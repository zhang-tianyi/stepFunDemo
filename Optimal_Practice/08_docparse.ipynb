{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文档解析开发指南\n",
    "\n",
    "阶跃星辰文档解析接口提供强大的文档解析能力，支持 PDF、Word 等多种格式，帮助开发者快速解析文档内容，并将其作为输入用于生成和推理。该接口提供从文档内容提取到生成应用的一站式解决方案，让基于文档内容的应用开发更加高效便捷。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 效果\n",
    "\n",
    "- **上传文件**：提供文档上传接口，支持多种文档格式，简化了基于文档的内容的输入。（注意：目前仅支持纯文本内容，图片或扫描形式的文本内容暂不支持）\n",
    "- **查询文件状态**：文档状态查询接口让开发者及时跟踪解析进度，随时了解文档是否成功解析。\n",
    "- **读取文件内容**：文档解析成功后，内容提取接口可快速获得文档内容，为后续基于文档内容的生成或分析提供数据基础。\n",
    "- **查询文件清单**：提供文件清单查询接口，帮助开发者获取已上传文档的列表，便于文档管理。\n",
    "- **删除文件**：支持文件删除功能，让开发者可以在文档处理完成后随时删除文档，方便管理文档存储。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开发步骤\n",
    "\n",
    "![doc-parser-flow](https://platform.stepfun.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fdoc-parser-flow.bcecd2cb.jpg&w=3840&q=75)\n",
    "\n",
    "通过【上传文件】【查询文件状态】【提取文件内容】+文本补全接口 即可获得完整基于文档内容的对话补全能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 上传文件\n",
    "\n",
    "上传文件到阶跃星辰文件服务，由阶跃星辰文件服务解析文件内容，用于后续生成和推理。\n",
    "\n",
    "- 上传时使用的 purpose 字段的值为 `file-extract`，用于文件解析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"file-Lc7s6mAn9U\",\n",
      "  \"object\": \"file\",\n",
      "  \"bytes\": 872404,\n",
      "  \"created_at\": 1761215390,\n",
      "  \"filename\": \"08_step-audio2.pdf\",\n",
      "  \"purpose\": \"file-extract\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 设置API密钥和文件路径\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "file_path = './media/08_step-audio2.pdf'  # 替换为您的文件路径\n",
    "\n",
    "# 设置请求头和URL\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}'\n",
    "}\n",
    "url = 'https://api.stepfun.com/v1/files'\n",
    "\n",
    "# 准备文件和请求参数\n",
    "files = {\n",
    "    'file': open(file_path, 'rb')\n",
    "}\n",
    "data = {\n",
    "    'purpose': 'file-extract'  # 指定用途为文件解析\n",
    "}\n",
    "\n",
    "# 发送请求\n",
    "response = requests.post(url, headers=headers, files=files, data=data)\n",
    "\n",
    "# 打印响应结果\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回结果示例:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"file-abc123\",\n",
    "  \"object\": \"file\",\n",
    "  \"bytes\": 12345,\n",
    "  \"created_at\": 1677610602,\n",
    "  \"filename\": \"example.pdf\",\n",
    "  \"purpose\": \"file-extract\",\n",
    "  \"status\": \"processing\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 查询文件状态\n",
    "\n",
    "在文件上传完成后，需要查询文件解析状态，确认文件解析是否完成，以便后续获取文件内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查次数 1, 状态: success\n",
      "文件处理完成！\n",
      "{\n",
      "  \"id\": \"file-Lc7s6mAn9U\",\n",
      "  \"object\": \"file\",\n",
      "  \"bytes\": 872404,\n",
      "  \"created_at\": 1761215391,\n",
      "  \"filename\": \"08_step-audio2.pdf\",\n",
      "  \"purpose\": \"file-extract\",\n",
      "  \"status\": \"success\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# 设置API密钥和文件ID\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "file_id = 'file-Lc7s6mAn9U'  # 替换为上传文件后获得的文件ID\n",
    "\n",
    "# 设置请求头和URL\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}'\n",
    "}\n",
    "url = f'https://api.stepfun.com/v1/files/{file_id}'\n",
    "\n",
    "# 轮询检查文件状态，直到处理完成\n",
    "max_retries = 10\n",
    "retry_count = 0\n",
    "\n",
    "while retry_count < max_retries:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    result = response.json()\n",
    "    print(f'检查次数 {retry_count + 1}, 状态: {result.get(\"status\", \"unknown\")}')\n",
    "    \n",
    "    # 如果文件处理完成，跳出循环\n",
    "    if result.get('status') == 'success':\n",
    "        print('文件处理完成！')\n",
    "        print(json.dumps(result, indent=2))\n",
    "        break\n",
    "    \n",
    "    # 如果文件处理失败，跳出循环\n",
    "    if result.get('status') == 'error':\n",
    "        print('文件处理失败！')\n",
    "        print(json.dumps(result, indent=2))\n",
    "        break\n",
    "    \n",
    "    # 等待一段时间后再次检查\n",
    "    retry_count += 1\n",
    "    time.sleep(2)  # 等待2秒\n",
    "\n",
    "if retry_count >= max_retries:\n",
    "    print('达到最大重试次数，文件可能仍在处理中。')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回结果示例，当status为success时，表示文件解析处理完成:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"file-abc123\",\n",
    "  \"object\": \"file\",\n",
    "  \"bytes\": 12345,\n",
    "  \"created_at\": 1677610602,\n",
    "  \"filename\": \"example.pdf\",\n",
    "  \"purpose\": \"file-extract\",\n",
    "  \"status\": \"success\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 提取文件内容\n",
    "\n",
    "当文件完成解析后，就可以使用提取文件内容接口获取文件内容，并在 Chat API 中，将内容作为对话的输入，生成对话补全。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-Audio 2 Technical Report\n",
      "StepFun Audio Team\n",
      "Abstract\n",
      "This paper presents Step-Audio 2, an end-to-end multi-modal large language modeldesigned for industry-strength audio understanding and speech conversation. Byintegrating a latent audio encoder and reasoning-centric reinforcement learning(RL), Step-Audio 2 achieves promising performance in automatic speech recognition(ASR) and audio understanding. To facilitate genuine end-to-end speech conversa-tion, Step-Audio 2 incorporates the generation of discrete audio tokens into languagemodeling, significantly enhancing its responsiveness to paralinguistic informationsuch as speaking styles and emotions. To effectively leverage the rich textual andacoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmentedgeneration (RAG) and is able to call external tools such as web search to mitigatehallucination and audio search to switch timbres. Trained on millions of hours ofspeech and audio data, Step-Audio 2 delivers intelligence and expressiveness acrossdiverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2achieves state-of-the-art performance on various audio understanding and conversa-tional benchmarks compared to other open-source and commercial solutions. Pleasevisit https://github.com/stepfun-ai/Step-Audio2 for more information.\n",
      "1Introduction\n",
      "With the rapid development of large language models and audio processing technology, largeaudio language models (LALMs) have demonstrated their superiority over conventional approachesin various speech and audio processing tasks. GPT-4o is first introduced and is pioneering thedevelopment of end-to-end speech interaction without intermediate textual conversions. Subse-quently, many open-sourced LALMs [9, 13, 16, 18, 21, 31, 32, 49, 71, 72, 74, 77] are emerged,advancing multi-modal large language model capabilities in various speech and audio domains.Among these approaches, Qwen-Audio [12] and Qwen2-Audio [13] perform audio analysis andgenerate textual responses to speech instructions. Qwen2.5-Omni [74] implements a thinker-talkerarchitecture to enable full-duplex I/O during speech conversations. More recently, Kimi-Audio [18]has achieved impressive results on multiple speech and audio understanding benchmarks. In parallel,we have introduced Step-Audio [32] and Step-Audio-AQAA [31], the first LALMs to unify speechunderstanding and generation through discrete audio tokens at a scale of 130 billion parameters.\n",
      "However, existing LALMs still face challenges in achieving natural and intelligent speech interaction.Previous LALMs such as Spirit LM [49] and GLM-4-Voice [77] mainly focus on aligning thesemantic information in speech inputs to text modal, neglecting the para-linguistic informationwhich is also crucial for intentional understanding. Although LALMs including Qwen-Audio [12],Qwen2-Audio [13] and Audio Flamingo series [24, 25, 43] are capable of comprehending suchinformation, they typically generate only textual outputs and fail to further utilize this capability\n",
      "arXiv:2507.16632v3  [cs.CL]  27 Aug 2025\n",
      "Step-Audio 2Technical Report\n",
      "AISHELL-2\n",
      "LibriSpeech\n",
      "test-clean\n",
      "MMAU Speech\n",
      "MMAU Sound\n",
      "MMAU Music\n",
      "StepEval-Audio-Paralinguistic\n",
      "CoVoST 2\n",
      "(S2TT, en<->zh)\n",
      "CVSS\n",
      "(S2ST, en<->zh)\n",
      "URO-Bench\n",
      "Basic-zh\n",
      "URO-Bench\n",
      "Basic-en\n",
      "95.7\n",
      "97.3\n",
      "97.6\n",
      "97.9\n",
      "98.2\n",
      "98.5\n",
      "97.1\n",
      "98.8\n",
      "64.6\n",
      "65.5\n",
      "70.6 76.9\n",
      "58.0\n",
      "79.078.1\n",
      "83.5\n",
      "51.8\n",
      "64.4\n",
      "65.9\n",
      "73.7\n",
      "43.5\n",
      "49.6\n",
      "44.2\n",
      "83.1\n",
      "29.6\n",
      "N/A\n",
      "35.4\n",
      "39.3\n",
      "23.7\n",
      "15.3\n",
      "30.9\n",
      "78.6\n",
      "73.6\n",
      "69.0\n",
      "83.3\n",
      "84.5\n",
      "60.0\n",
      "70.6\n",
      "83.9\n",
      "GPT-4o AudioKimi-AudioQwen-OmniStep-Audio 2\n",
      "Figure 1: Performance comparision of GPT-4o Audio1, Kimi-Audio2, Qwen-Omni3 and Step-Audio 2 and on variousbenchmarks.\n",
      "to produce coherent and expressive responses in speech conversations. Moreover, due to thecomplexities of multi-modal modeling, existing LALMs frequently suffer from hallucination andoffer limited choices of timbres and speaking styles [16, 18], lacking access to real-world textualand acoustic knowledge.\n",
      "To address these issues and step into the next generation of multi-modal large language models,we present Step-Audio 2, an end-to-end large audio language model with industry-strength audioperception and speech interaction. Step-Audio 2 directly processes raw audio as input and outputsdiscrete text and audio tokens and has fewer parameters than Step-Audio [32]. Beyond capturingsemantic information in speech, the model also comprehends para-linguistic and non-vocal informa-tion in audio. By leveraging chain-of-thought (CoT) reasoning and reinforcement learning (RL),Step-Audio 2 further utilizes such multi-modal information to generate expressive speech responsescoherent to different conversation scenarios. To ground the model with real-world knowledge, Step-Audio 2 incorporates retrieval-augmented generation (RAG) and the capability to utilize variousexternal tools, including web search and audio search, to provide more reliable and expressiveresponses. Specifically, we present an audio search as a tool unique to LALMs, enabling seamlessspeech retrieval via voice instructions and allowing the model to switch timbres and speaking stylesbased on the retrieved speech.\n",
      "1GPT-4o Audio is evaluated with gpt-4o-transcribe for ASR and gpt-4o-audio-preview-2025-06-03 for others via official API.2Kimi-Audio is excluded from translation evaluations since it consistently ignores prompts.3Qwen-Omni is evaluated with Qwen2.5-Omni for MMAU and speech-to-text translation, and qwen-omni-turbo-2025-03-26 forothers via official API.\n",
      "2\n",
      "Step-Audio 2Technical Report\n",
      "4’’\n",
      "Could you talk like a little girl?\n",
      "Yay! Now I'm a happy little girl!What do you wanna talk about?\n",
      "10’’\n",
      "👧\n",
      "4’’\n",
      "Could you talk like a grandpa?\n",
      "Alright then, let this old manhave a chat with ya. What’s up?\n",
      "10’’\n",
      "👴\n",
      "4’’\n",
      "Tell me about the Great Wall.\n",
      "Oh, the Great Wall is like agiant dragon ...\n",
      "10’’\n",
      "👴\n",
      "4’’\n",
      "Answer in the default voice,what is the capital of China?\n",
      "The capital of China is Beijing.\n",
      "10’’\n",
      "4’’\n",
      "(Shanghai dialect) What’s your\n",
      "name?\n",
      "My name is Step-Audio 2.\n",
      "10’’\n",
      "4’’\n",
      "(Sichuan dialect) Whichcompany developed you?\n",
      "I'm developed by StepFun.\n",
      "10’’\n",
      "4’’\n",
      "What day is it today?\n",
      "Today is Thursday.\n",
      "10’’\n",
      "📅\n",
      "4’’\n",
      "Please check today’s S&P 500Index.\n",
      "Today’s S&P 500 Index is6297.36.\n",
      "10’’\n",
      "🔍\n",
      "4’’\n",
      "Translate, never gonna give youup, never gonna let you down.\n",
      "永远不会放弃你，永远不会让你失望。\n",
      "10’’\n",
      "4’’\n",
      "请翻译一下，长城是一座伟大的建筑。\n",
      "The Great Wall is a magnificentstructure.\n",
      "10’’\n",
      "4’’\n",
      "Could you say that sadly?\n",
      "The Great Wall is a magnificentstructure.\n",
      "10’’\n",
      "😭\n",
      "4’’\n",
      "Please say it in a whisper.\n",
      "The Great Wall is a magnificentstructure.\n",
      "10’’\n",
      "🤫\n",
      "10’’\n",
      "Good evening… I’m feeling a bit unsettled.I wonder if I could talk to you.\n",
      "Of course, I’m here for you. It sounds like you’re carrying a lot of emotions right now—that can feel overwhelming. Would you like toshare what’s on your mind, or just take a moment to unwind? There’s no rush; I’ll listen whenever you’re ready.\n",
      "25’’\n",
      "Young male, 20-25 years old, speaking in a gentle and slightly thoughtful tone, with a steady and natural pace, slightbackground noise, using standard Mandarin.\n",
      "Alright, the user has come to me now, saying “Good evening, I‘m feeling a bit unsettled, and I’d like to talk.” First, I need to showunderstanding and acceptance to make him feel safe. He may be in a state of anxiety or unease, so my response should begentle, patient, and give him ample space to express himself. The user mentioned feeling “a bit unsettled,” which suggests hemay have many emotions tangled together and isn’t sure where to start. I need to guide him in slowly untangling these emotionsrather than rushing to solve the problem. At this point, empathy is key—letting him know his feelings are valid. Finally, I mustensure not to end the conversation prematurely—let the user take the lead entirely. If he doesn’t want to delve deeper for now,respect his pace, offer support, and wait. Maintain a soft and caring tone, making him feel heard and supported, so he’s morewilling to open up further.\n",
      "Thinking\n",
      "Figure 2: Illustration of the applications of Step-Audio 2 across various speech conversation scenarios.\n",
      "To ensure its intelligence and expressiveness in diverse conversational scenarios, we carefully designa multi-stage training strategy to train Step-Audio 2 on 680 billion tokens of text data and 8 millionhours of real and synthesized audio data. Evaluation results shown in Figure 1 demonstrate thatStep-Audio 2 achieves state-of-the-art performance in a series of audio tasks, including automaticspeech recognition (ASR) on multiple languages, audio understanding, speech-to-speech translationand speech-to-speech conversation. Typical usages of Step-Audio 2 are also illustrated in Figure 2.\n",
      "3\n",
      "Step-Audio 2Technical Report\n",
      "2Related Work\n",
      "2.1Speech and audio understanding\n",
      "Recent advances in large language models (LLMs) [4, 29, 50, 51] have extended their applicationto a wide range of speech and audio understanding tasks, such as audio captioning, sound eventdetection, automatic speech recognition, audio classification, and audio-driven creative generation.A prevalent approach [13, 17, 27, 28, 48, 61] involves pairing speech encoders with lightweight,trainable adapters that project audio features into a textual embedding space compatible with LLMs.Building on this foundation, recent studies have further explored how to incorporate paralinguisticinformation such as emotion, intonation and speaker style, enabling LLMs to move beyond purelinguistic comprehension. For instance, ParalinGPT [48] focuses on enhancing a powerful text-basedlanguage model by integrating continuous speech embeddings, enabling it to capture paralinguisticsignals such as emotion and prosody. SALMONN [61] adopts a multi-modal strategy by freezingspeech encoders Whisper [53] and BEATs [10], and connecting their outputs to an LLM via awindow-level Q-Former, enabling joint modeling of linguistic and acoustic features. Seed-ASR [5]integrates LUISE-based speech representations with instructions and context, using context-awareSFT to capture semantic information. AudioPaLM [57] combines PaLM-2 [2] and AudioLM [7],unifying linguistic knowledge with paralinguistic features like speaker identity and intonation. LLM-based approaches [12, 13] increasingly rely on pretrained audio encoders such as Wav2Vec [3],HuBERT [30], Whisper [53], and WavLM [11] to extract rich semantic representations from speech.At the same time, the extensive text knowledge and contextual reasoning capabilities stored inLLMs can provide valuable semantic guidance for understanding tasks.\n",
      "2.2Text-to-speech synthesis\n",
      "Text-to-Speech (TTS) technology has made remarkable strides in recent years, evolving fromtraditional concatenative and statistical parametric approaches [52, 55, 59, 68] to codec-based TTSsystems. Codec language models leverage a speech codec to extract discrete representations ofspeech [15, 16, 36, 60, 73, 76, 81, 82] and utilize either autoregressive [18, 80] or masked languagemodels [67] to predict the corresponding speech tokens. These tokens are then synthesized intowaveforms using codec vocoders. VALL-E [64] marked a significant breakthrough in this area. Ituses an autoregressive model to generate coarse codec codes, followed by a non-autoregressivemodel for the fine codes. Unlike VALL-E, which predicts acoustic tokens from phonemes andrequires transcripts, SPEAR-TTS [40] uses a two-stage architecture with self-supervised audioprompts to clone unseen voices from just 3 seconds of speech. SparkTTS [65] introduces BiCodec, asingle-stream speech codec that encodes linguistic content as compact semantic tokens and speakercharacteristics as fixed-length global tokens. Instead of relying on non-autoregressive models topredict residual discrete codes, methods like TorToiseTTS [6], CosyVoice [20], CosyVoice 2 [19],MiniMax-Speech [79] and SEED-TTS [1] adopt diffusion or flow-matching techniques as a secondstage to reconstruct mel-spectrograms or continuous representations enriched with fine-grainedacoustic and semantic details. Recent work, Kimi-Audio [18] combines Whisper features andsemantic tokens for efficient modeling, with dual heads and a flow-matching detokenizer plusBigVGAN [46] for low-latency, expressive synthesis.\n",
      "4\n",
      "Step-Audio 2Technical Report\n",
      "2.3Speech-to-speech translation\n",
      "Speech-to-speech translation (S2ST) is a crucial technology for eliminating communication bar-riers across languages. Traditional S2ST systems [62, 70] typically adopt a cascaded pipelineconsisting of automatic speech recognition (ASR), machine translation (MT), and TTS modules.Earlier studies [38, 39, 44, 45] have shifted toward direct approaches that bypass intermediatetextual representations, aiming for lower latency and better preservation of prosody and speakercharacteristics. Two main types of direct S2ST methods have emerged, which are known as speech-to-spectrogram translation and speech-to-unit translation. Both directly generate target speechrepresentations from the source speech without relying on textual transcriptions. A representative ofthe former is Translatotron [38], the first end-to-end model to translate source speech directly intotarget spectrograms. Translatotron 2 [39], further improves translation quality through a two-passdecoding mechanism [45]. In contrast, speech-to-unit models predict discrete acoustic tokens ratherthan spectrograms, which are typically extracted using self-supervised speech encoders such asHuBERT [30] or WavLM [11]. For instance, TransVIP [44] employs a joint encoder-decoderarchitecture that first generates target text and residual vector quantization (RVQ) codes in the initiallayer, followed by a non-causal language model that refines RVQ predictions in subsequent layers.\n",
      "2.4Speech-to-text and speech-to-speech conversation\n",
      "Based on whether the LLM can directly understand and generate speech representations, existingsystems can be categorized into end-to-end large audio language models and cascaded large audiolanguage models. The former directly models audio inputs and outputs within a unified framework,while the latter relies on a modular pipeline involving separate ASR, LLM, and TTS components.Traditional speech-to-text and speech-to-speech systems typically adopt a cascaded architecture, asexemplified by AudioGPT [33] and Spoken-LLM [47]. However, the ASR + LLM + TTS pipelineincurs high latency and modular mismatches. This has spurred interest in unified end-to-endarchitectures for faster and more seamless integration. A major milestone in this direction is GPT-4o [34], which supports direct end-to-end speech interaction without requiring intermediate textualconversions. Recently, several new end-to-end LALMs [16, 21, 71, 72, 74] for speech-to-speechconversation have emerged. For instance, Moshi [16] improves efficiency with an RQ-Transformerthat generates text and audio tokens simultaneously. Similarly, Mini-Omni [71] generates speechand text responses in parallel, following a strategy similar to MusicGen [14], which enables lowerfirst-token latency compared to interleaved generation designs. LUCY [22] builds on the Mini-Omniarchitecture with enhancements for emotional expressiveness, naturalness, and informativenessin speech generation. It utilizes curated synthetic data and optimizes the training and decodingpipelines to handle multi-turn dialogue and function-call scenarios. Mini-Omni2 [71] furtherextends Mini-Omni framework by integrating multimodal understanding and full-duplex interactioncapabilities. LLaMA-Omni [21] introduces a streaming, non-autoregressive speech decoder basedon Connectionist Temporal Classification, enabling direct and efficient generation of discrete audiotokens without relying on step-by-step prediction. Freeze-Omni [66], on the other hand, freezes theLLM parameters during training, preserving its original capabilities while achieving low-latencyspeech-to-speech interaction through streaming and decoder integration. Qwen2.5-Omni [74]supports multimodal input and simultaneous text-speech output via a thinker-talker architecture,using TMRoPE for improved audio-visual synchronization through explicit temporal encoding.\n",
      "5\n",
      "Step-Audio 2Technical Report\n",
      "Audio\n",
      "Detokenizer\n",
      "❄\n",
      "Output text-audio interleaved tokens\n",
      "…\n",
      "Audio\n",
      "Encoder\n",
      "Adaptor\n",
      "❄\n",
      "Input audio features\n",
      "🔥\n",
      "Input audioOutput speech\n",
      "LLM Decoder\n",
      "History information\n",
      "🔥\n",
      "…\n",
      "Discrete text token\n",
      "Discrete audio token\n",
      "Latent audio feature\n",
      "………\n",
      "Figure 3: Architecture of the Step-Audio 2.\n",
      "3Methodology\n",
      "3.1Architecture\n",
      "Different from our previous Step-Audio [32], Step-Audio 2 further integrates the generation ofaudio tokens into language modeling, achieves end-to-end audio perception and generation. Asshown in Figure 3, Step-Audio 2 consists of an audio encoder, an audio adaptor, an LLM decoderand an audio detokenizer.\n",
      "The audio encoder is pretrained on various speech and audio understanding tasks including ASR,speaker age and gender prediction, audio event detection, etc. The audio encoder has an outputframe rate of 25 Hz and is frozen during the entire training process. An audio adaptor with adownsampling rate of 2 is employed to connect the audio encoder to LLM, thereby reducing theoutput frame rate of the audio encoder to 12.5 Hz.\n",
      "The LLM decoder directly takes the latent audio features from the audio adaptor as input, andoutputs an interleaved sequence of discrete text and audio tokens. We employ the tokenizer fromCosyVoice 2 [19] as the audio tokenizer. And the text and audio tokens are interleaved [31, 32, 77]at a fixed ratio and padded at the end to meet the ratio. The audio tokens are then extracted from theinterleaved sequence and consumed by the audio detokenizer to generate the output waveform. Theinput audio features and output interleaved sequences are then pre-filled as the history informationfor the next round of conversation.\n",
      "To provide more accurate responses and expand interactive capabilities, we design tools to retrieveaudio, current date and time, weather forecast and web content directly with explicit or implicitvoice inputs. Notably, we propose the audio search tool, a novel tool with a voice library of hundredsof thousands of speeches with their corresponding transcriptions and descriptions. With the retrievedspeech from audio search, Step-Audio 2 is able to mimic the speaking style or switching timbre\n",
      "6\n",
      "Step-Audio 2Technical Report\n",
      "according to the speech. During inference, the retrieved information is appended after the inputaudio features before generating speech outputs.\n",
      "Similar to Step-Audio [32] and Step-Audio-AQAA [31] , Step-Audio 2’s audio detokenizer alsoconsists of a Flow Matching module and a HiFi-GAN [42] vocoder. The Flow-Matching modulegenerates Mel spectrograms from the output audio tokens, while the vocoder further converts theMel spectrograms into waveforms. For Flow-Matching, we incorporate a CNN-based encoder layerafter each self-attention module within the transformer block and train the model on 200,000 hoursof high-quality speech. This enhancement significantly improves its Mel spectrogram reconstructioncapability, leading to substantial gains in both pronunciation accuracy and timbre similarity.\n",
      "Step-Audio 2 employs the same deployment infrastructure used in Step-Audio [32] and Step-Audio-AQAA [31], which includes a voice activity detection (VAD) module to filter out input speechesand achieves real-time voice conversation.\n",
      "3.2Pre-training\n",
      "Step-Audio 2 model is initialized with a textual LLM and then continually pre-trained on 1.356Ttokens of textual and audio data over 21 days.\n",
      "We first utilize 100B tokens of ASR data to facilitate effective alignment between speech and textfeature spaces within the adaptor. During this phase, both the audio encoder and LLM are frozen,with only the adaptor being trained. We conduct training for 12K steps at an 8,192 sequence length.And the learning rate decays from 10−4 to 2 × 10−5.\n",
      "We then extend the tokenizer of the textual LLM with 6.6K audio tokens. To properly embed thenew audio tokens and preserve the model’s textual capabilities, the model is then trained on 128Btokens of text data and 128B tokens of audio data. Specifically, audio data includes 80B, 32Band 16B tokens of TTS, speech-to-speech conversation and utterance-level text-speech interleavedcontinuation data respectively. The sequence length is increased to 16,384. And the learning ratesof the LLM, adaptor, embedding layer and output layer are set to 2 × 10−5, 5 × 10−5, 5 × 10−5,and 4 × 10−5 respectively.\n",
      "We then introduce our main pre-training process and further train the model on another 800Btokens of text and audio data. We unify the learning rates to 2 × 10−5 and employ 400B tokensof textual data and 42B, 120B, 8B, 30B, 5B, 45B and 150B tokens of ASR, TTS, speech-to-text translation, text-to-speech translation, speech-to-text continuation, utterance-level text-speechinterleaved continuation and speech-to-speech conversation data respectively.\n",
      "We finally employ 200B tokens of high-quality text and audio data to introduce a wider array oftasks and cooldown the model. We employ 24.6B, 12.4B, 2.4B, and 3.6B tokens of audio datafor multilingual and dialectal ASR, TTS, paralinguistic information understanding, speech-to-texttranslation respectively. Besides, we develop a conversational speech synthesis pipeline to synthesize6B, 15B and 36B tokens of audio data for speech-to-speech translation, utterance-level text-speechinterleaved conversation and speech-to-speech conversation. To ensure the vocal diversity in thesynthesized speech, the system references a library of approximately 50k unique speakers. Webalance the audio data with 100B tokens of high-quality text data and the learning rate decays from2 × 10−5 to 5 × 10−6.\n",
      "7\n",
      "Step-Audio 2Technical Report\n",
      "After this comprehensive pre-training procedure, the model has acquired strong audio understandingand generation capabilities while maintaining its textual performance inherited from the initialtextual LLM.\n",
      "3.3Supervised fine-tuning\n",
      "We subsequently perform a large-scale, multi-task supervised fine-tuning (SFT) procedure [69]to instruct the model to follow human intention in fluid conversations and master core tasks. Weselect audio data from open-source and proprietary data to ensure broad coverage and high quality.The model is trained on 4B tokens of text and audio data for a single epoch. And the learning ratedecays from 10−5 to 10−6.\n",
      "Specifically, we leverage extensive corpora such as GigaSpeech [8], WenetSpeech [78], and other in-house data to enhance the model’s performance in multilingual and multi-dialect ASR scenarios. Wereformat existing datasets for audio event classification and audio captioning, such as AudioSet [23]and AudioCaps [41], into speech question-answer pairs for audio understanding. To captureparalinguistic information beyond just semantics, we introduce a detailed speech captioning taskand build an in-house dataset, requiring the model to generate comprehensive textual descriptionsencompassing 11 paralinguistic and environmental aspects.\n",
      "We employ high-quality, professionally labeled data collected in-house for TTS. We utilize theChinese-to-English and English-to-Chinese subsets from the CoVoST 2 [63] dataset for speech-to-speech translation.\n",
      "We leverage high-quality in-house textual data for classic text-to-text conversation. Multiple LLMsare then employed to rewrite these text conversations as dialogue scripts with a more natural,colloquial style. We randomly insert emotion and speed instructions into the generated scriptsto enable basic emotion and speaking style control. The scripts are then synthesized into speechconversations using our conversation synthesis pipeline.\n",
      "We construct approximately 1K dialogue scripts in text for each type of external tools. Withinthese scripts, instructions with explicit or implicit tool invocation intentions and their correspondingstatements are inserted into common dialogues. The scripts are then synthesized into speechconversations using our conversation synthesis pipeline.\n",
      "Besides, we construct and employ two reasoning-centric datasets during SFT to cold-start thesubsequent reinforcement learning process. First, we build a dataset to enable and robust audiounderstanding in complex acoustic scenarios, by combining multiple audios from AudioSet andAudioCaps, thereby creating intricate acoustic environments. To better address and respond to theparalinguistic information in speech conversations, we synthesize a speech conversation dataset withour conversation synthesis pipeline, based on dialogue scripts with appropriate emotion descriptionsgenerated from textual LLMs. Subsequently, a textual LLM with reasoning capabilities is employedto produce question–answer pairs with explicit step-by-step reasoning traces, according to the audiomixing recipes or the generated dialogue scripts.\n",
      "3.4Reinforcement learning\n",
      "To enhance the model’s reasoning capabilities in audio understanding and speech interaction, weimplement a multi-stage reinforcement learning strategy. We leverage our reasoning-centric datasetsfrom SFT and utilize two stages of proximal policy optimization (PPO) [54] to optimize reasoning\n",
      "8\n",
      "Step-Audio 2Technical Report\n",
      "efficiency for real-time audio engagement. In the first stage, a binary reward function is employedto limit the thinking sequence length to a predefined maximum. This reward function assigns avalue of 1 for reasoning that is appropriately concise (neither empty nor excessively long) and 0otherwise. Training is conducted for 60 iterations with a global batch size of 64, using an actorlearning rate of 1 × 10−6 and a critic learning rate of 2.5 × 10−6. The second stage transitions frombinary rewards to learned preference scoring, utilizing a trained reward model to evaluate responsequality. This stage involves an additional 120 iterations while maintaining the same batch size andlearning rate settings. Finally, we incorporate group relative policy optimization (GRPO) [54] for400 iterations to further improve the model’s audio perceptual abilities.\n",
      "4Evaluation\n",
      "4.1Automatic speech recognition\n",
      "As the most critical component of audio understanding and speech interaction, we first evaluate themodel’s capability in automatic speech recognition. We evaluate Step-Audio 2 across six Chinesetest sets, four English test sets, three multilingual test sets (Japanese, Cantonese, Arabic), and sixin-house Chinese dialect and accented Mandarin test sets. For comparative analysis, we utilize top-performing models from both open-source and commercial domains as baselines, including DoubaoLLM ASR1, GPT-4o Transcribe2, Kimi-Audio [18], and Qwen-Omni. We prefer GPT-4o Transcribethan GPT-4o Audio since the formers provide stronger results. Notably, Doubao LLM ASR andGPT-4o Transcribe represent specialized ASR systems that achieve leading-edge performance.\n",
      "We evaluate all the models without specifying language3 and summarize the results in Table 1. Step-Audio 2 outperforms existing open-source and commercial ASR models in both general Englishand Chinese recognition, achieving an average word error rate (WER) of 3.14% on English and anaverage character error rate (CER) of 3.08% on Chinese test sets. Moreover, Step-Audio 2 offerscomparable results to GPT-4o Transcribe on Arabian and Japanese recognition, to Qwen-Omni onCantonese recognition, demonstrating its capability in multilingual speech recognition. In addition,Step-Audio 2 achieves the lowest average CER among 4 in-house Chinese accented Mandarin and2 dialect test sets. These results highlight the superiority of Step-Audio 2 in understanding thesemantic information in speech.\n",
      "4.2Paralinguistic information understanding\n",
      "We then evaluate how Step-Audio 2 understands the paralinguistic information in speech beyondthe semantic information. To this end, we introduce StepEval-Audio-Paralinguistic, a speech-to-speech benchmark that evaluates the model’s understanding of paralinguistic information across 11dimensions using single-turn question answering.\n",
      "StepEval-Audio-Paralinguistic comprises 550 speech samples evenly distributed across 11 tasks.We initially collect 400 Chinese speech clips for 8 of these tasks from public podcast recordings,encompassing gender, age, timbre, emotion, pitch, rhythm, speaking speed, speaking style, andvocal activity prediction or description. For sound event, scenario, and vocal sound detection or\n",
      "1Doubao LLM ASR refers to https://www.volcengine.com/docs/6561/13548682GPT-4o Transcribe is evaluated using its latest model, gpt-4o-transcribe, via its official API.3We evaluate without specifying language to ensure a fair comparison. Notably, Qwen-Omni lacks a language-independenttesting approach, specifying language may yield better results.\n",
      "9\n",
      "Step-Audio 2Technical Report\n",
      "Table 1: Comparison between Doubao LLM ASR, GPT-4o Transcribe, Kimi-Audio, Qwen-Omni and Step-Audio 2, oncharacter (for Chinese, Cantonese and Japanese) and word (for Arabian and English) error rates among multiple ASRtest sets. N/A indicates that the language is not supported.\n",
      "CategoryTest setDoubaoLLM ASRGPT-4oTranscribeKimi-AudioQwen-OmniStep-Audio 2\n",
      "English\n",
      "Common Voice9.209.307.838.335.95FLEURS English7.222.714.475.053.03LibriSpeech clean2.921.751.492.931.17LibriSpeech other5.324.232.915.072.42\n",
      "Average6.174.504.185.353.14\n",
      "Chinese\n",
      "AISHELL0.983.520.641.170.63AISHELL-23.104.262.672.402.10FLEURS Chinese2.922.622.917.012.68KeSpeech phase16.4826.805.116.453.63WenetSpeech meeting4.9031.405.216.614.75WenetSpeech net4.4615.715.935.244.67\n",
      "Average3.8114.053.754.813.08\n",
      "FLEURS ArabianN/A11.72N/A25.1314.22MultilingualCommon Voice yue9.2011.1038.907.897.90FLEURS JapaneseN/A3.27N/A10.493.18\n",
      "Anhui accent8.8350.5522.1718.7310.61Guangdong accent4.997.833.764.033.81Guangxi accent3.377.094.293.354.11In-houseShanxi accent20.2655.0334.7125.9512.44Sichuan dialect3.0132.855.265.614.35Shanghai dialect47.4989.5882.9058.7417.77\n",
      "Average14.6640.4925.5219.408.85\n",
      "description, we source 50 event-related, 50 environmental, and 50 vocal sounds from AudioSet [23],CochlScene [35], and VocalSound [26], respectively. All original recordings are shorter than 30seconds and uniformly resampled to 24,000 Hz, with annotations provided by professional groupsin open-set natural language.\n",
      "We then generate textual questions and answers based on the ground-truth annotations for each taskwith textual LLMs. For the first 8 tasks, we use the input speech as a prompt to clone a synthesizedquestion speech and randomly concatenate the question before or after the original speech. Forthe remaining 3 tasks, we further mix these audios with synthesized speeches before questionconcatenation, creating more challenging test samples.\n",
      "We also establish an automatic evaluation protocol for StepEval-Audio-Paralinguistic, which initiallytranscribes model outputs into text using ASR, followed by automatic judgment with a textual LLM.More information, along with the complete StepEval-Audio-Paralinguistic test set and evaluationcode, is available at https://github.com/stepfun-ai/Step-Audio2 to foster further researchon paralinguistic information understanding.\n",
      "We evaluate GPT-4o Audio, Kimi-Audio, Qwen-Omni, Step-Audio-AQAA, and Step-Audio 2 usingthe StepEval-Audio-Paralinguistic benchmark, with results presented in Table 2. The experimentalresults highlight the comprehensive capabilities of Step-Audio 2 in understanding various paralin-guistic information, achieving an average accuracy of 83.09, which is a significant improvementover other baseline models.\n",
      "10\n",
      "Step-Audio 2Technical Report\n",
      "Table 2: Comparison between GPT-4o Audio, Kimi-Audio, Qwen-Omni, Step-Audio-AQAA and Step-Audio 2 onStepEval-Audio-Paralinguistic.\n",
      "ModelAvg.GenderAgeTimbreScenarioEvent\n",
      "GPT-4o Audio43.451842342214Kimi-Audio49.649450103048Qwen-Omni44.184050162842Step-Audio-AQAA36.917066181414Step-Audio 283.0910096827860\n",
      "ModelEmotionPitchRhythmSpeedStyleVocal\n",
      "GPT-4o Audio824060586444Kimi-Audio665640445454Qwen-Omni763254505048Step-Audio-AQAA40384854440Step-Audio 2868286888868\n",
      "4.3Audio understanding\n",
      "We then assess Step-Audio 2’s general audio comprehension across sound, speech, and music usingthe latest version of the MMAU benchmark [58]1.\n",
      "As baselines, we employ Audio Flamingo 3, Gemini 2.5 Pro, GPT-4o Audio, Kimi-Audio, Omni-R1 [56], Qwen2.5-Omni, and Step-Audio-AQAA. We obtain the reported results for Audio Flamingo3, Omni-R1, and Qwen2.5-Omni from their original papers. The results of Gemini 2.5 Pro areobtained from the official website of MMAU. And we re-evaluate GPT-4o Audio, Kimi-Audio andStep-Audio-AQAA due to the recent update of the MMAU benchmark.\n",
      "The results are summarized in Table 3. Step-Audio 2 achieves the highest average score of 78.0,followed by Omni-R1 and Audio Flamingo 3, both of which are specialized approaches in audiounderstanding. Specifically, Step-Audio 2 yields the best results in sound and speech tracks and onpar results with the best in music track, demonstrating its versatility and robustness across differentaudio domains.\n",
      "Table 3: Comparison between Audio Flamingo 3, Gemini 2.5 Pro, GPT-4o Audio, Kimi-Audio, Omni-R1, Qwen2.5-Omni, Step-Audio-AQAA and Step-Audio 2 on MMAU.\n",
      "ModelAvg.SoundSpeechMusic\n",
      "Audio Flamingo 373.176.966.173.9Gemini 2.5 Pro71.675.171.568.3GPT-4o Audio58.158.064.651.8Kimi-Audio69.679.065.564.4Omni-R177.081.776.073.4Qwen2.5-Omni71.578.170.665.9Step-Audio-AQAA49.750.551.447.3Step-Audio 278.083.576.973.7\n",
      "4.4Speech translation\n",
      "We evaluate the model’s bidirectional Chinese-English speech translation capabilities using twobenchmarks: speech-to-text translation (S2TT) on CoVoST 2 [63] and speech-to-speech translation\n",
      "1MMAU v05.15.25 test-mini\n",
      "11\n",
      "Step-Audio 2Technical Report\n",
      "(S2ST) on CVSS [37]. Additionally, we use the reported results of Qwen2.5-Omni for CoVoST 2,while for CVSS, we employ Qwen-Omni as a baseline. Kimi-Audio is excluded because itconsistently ignores prompts and performs ASR instead of translation. Using BLEU as the evaluationmetric, the results in Table 4 demonstrate that Step-Audio 2 achieves superior performance inChinese-English bidirectional translations, obtaining the highest average score on both the CoVoST 2and CVSS test sets.\n",
      "Table 4: Comparison of BLEU scores between GPT-4o Audio, Qwen2.5-Omni, Qwen-Omni, Step-Audio-AQAA andStep-Audio 2 on speech-to-text and speech-to-speech translation.\n",
      "ModelCoVoST 2 (Speech-to-text translation)\n",
      "Avg.English-to-ChineseChinese-to-English\n",
      "GPT-4o Audio29.6140.2019.01Qwen2.5-Omni35.4041.4029.40Step-Audio-AQAA28.5737.7119.43Step-Audio 239.2649.0129.51\n",
      "ModelCVSS (Speech-to-speech translation)\n",
      "Avg.English-to-ChineseChinese-to-English\n",
      "GPT-4o Audio23.6820.0727.29Qwen-Omni15.358.0422.66Step-Audio-AQAA27.3630.7423.98Step-Audio 230.8734.8326.92\n",
      "4.5Tool calling\n",
      "To address the gap in the availability of suitable test sets for tool calling in speech conversations, weintroduce StepEval-Audio-Toolcall, a test set that evaluates the model’s ability in tool invocation,selection and parameter extraction under Chinese speech conversations.\n",
      "We employ a textual LLM to generate 200 multi-turn dialogue scripts for each kind of tool. Eachscript contains 3-6 turns of inputs and outputs, in which previous turns may or may not include toolcalling statements, but the final input must contain a calling intention to a specific external tool.We then balance the samples with an equal number of negative samples for each kind of tools, inwhich the final speech input either has no tool calling intention or intention to call on other kindsof tools. Subsequently, we synthesize these scripts into speeches with our conversation synthesispipeline. And we propose an automatic evaluation protocol to employ Qwen3-32B to automaticallyexamine the output and tool calling statements. We release StepEval-Audio-Toolcall includingthe original scripts, synthesized speech conversations and the corresponding evaluation script inhttps://github.com/stepfun-ai/Step-Audio2.\n",
      "Despite that there is no other LALM that provides custom tool calling, we employ Qwen3-32B asa baseline to illustrate how Step-Audio 2 manages external tools in comparison to textual LLMs.As shown in Table 5, Step-Audio 2 achieves on par with tool calling accuracy with textual LLMseven with speech input. Notably, Step-Audio 2 significantly outperforms Qwen3-32B in accuratelycalling our innovative audio search tool, highlighting its specialty as a multi-modal LLM thantextual LLMs.\n",
      "12\n",
      "Step-Audio 2Technical Report\n",
      "Table 5: Comparison between Step-Audio 2 and Qwen3-32B on StepEval-Audio-Toolcall. †Qwen3-32B is evaluatedwith text inputs. ‡Date and time tools have no parameter.\n",
      "ModelObjectiveMetricAudio searchDate & Time‡WeatherWeb search\n",
      "TriggerPrecision / Recall67.5 / 98.598.4 / 100.090.1 / 100.086.8 / 98.5Qwen3-32B†TypeAccuracy100.0100.098.598.5ParameterAccuracy100.0N/A100.0100.0\n",
      "TriggerPrecision / Recall86.8 / 99.596.9 / 98.492.2 / 100.088.4 / 95.5Step-Audio 2TypeAccuracy100.0100.090.598.4ParameterAccuracy100.0N/A100.0100.0\n",
      "4.6Speech-to-speech conversation\n",
      "We finally employ URO-Bench [75] to evaluate Step-Audio 2 and other open-source and commercialLALMs, including GPT-4o Audio, Kimi-Audio, Qwen-Omni, and Step-Audio-AQAA. URO-Benchconsists of multiple datasets on two difficulty tracks, evaluating the model’s understanding, reasoningand oral conversation abilities, such as ASR, instruction following, commonsense knowledge,mathematics, and speech naturalness, emotion and speaking styles expressions. We follow the ASR-mediated procedure in URO-Bench for evaluation, employing Whisper for ASR and GPT-4o-minifor automatic judging.\n",
      "As demonstrated in Table 6, Step-Audio 2 significantly outperforms existing large audio languagemodels, including GPT-4o Audio, in Chinese speech-to-speech conversation scenarios, achievingthe highest average scores of 83.32 on the basic track and 68.25 on the pro track. In Englishspeech-to-speech conversations, while Step-Audio 2 is slightly outperformed by GPT-4o Audio, itprovides very competitive results and exceeds the other approaches.\n",
      "Table 6: Comparison between GPT-4o Audio, Kimi-Audio, Qwen-Omni, Step-Audio-AQAA and Step-Audio 2 on theURO-Bench. U. R. O. stands for understanding, reasoning, and oral conversation, respectively.\n",
      "ModelLanguageBasicPro\n",
      "Avg.U.R.O.Avg.U.R.O.\n",
      "GPT-4o Audio\n",
      "Chinese\n",
      "78.5989.4065.4885.2467.1070.6057.2270.20Kimi-Audio73.5979.3464.6679.7566.0760.4459.2976.21Qwen-Omni68.9859.6669.7477.2759.1159.0159.8258.74Step-Audio-AQAA74.7187.6159.6381.9365.6174.7647.2968.97Step-Audio 283.3291.0575.4586.0868.2574.7863.1865.10\n",
      "GPT-4o Audio\n",
      "English\n",
      "84.5490.1875.9090.4167.5160.6564.3678.46Kimi-Audio60.0483.3642.3160.3649.7950.3240.5956.04Qwen-Omni70.5866.2969.6276.1650.9944.5163.8849.41Step-Audio-AQAA71.1190.1556.1272.0652.0144.2554.5459.81Step-Audio 283.9092.7276.5184.9266.0764.8667.7566.33\n",
      "5Conclusion\n",
      "We introduce Step-Audio 2, an end-to-end large audio language model designed for enterprisespeech and audio understanding, as well as intelligent speech interaction. Step-Audio 2 leverages alatent audio encoder and reinforcement learning to enhance its speech and audio comprehensioncapabilities. Furthermore, by integrating the generation of discrete audio tokens into language mod-eling, Step-Audio 2 achieves genuine end-to-end speech interaction and improves its responsiveness\n",
      "13\n",
      "Step-Audio 2Technical Report\n",
      "to paralinguistic information, such as speaking styles and emotions. Step-Audio 2 is also capable ofutilizing external tools including web search and audio search for multi-modal RAG. Trained on8 million hours of speeches and audios, Step-Audio 2 demonstrates state-of-the-art performanceacross various tasks, including ASR, audio understanding, speech translation, and general speechconversation, outperforming both open-source and commercial solutions.\n",
      "References\n",
      "[1]Philip Anastassiou et al. “Seed-tts: A family of high-quality versatile speech generation models”. In: arXivpreprint arXiv:2406.02430 (2024).\n",
      "[2]Rohan Anil et al. PaLM 2 Technical Report. 2023. arXiv: 2305.10403 [cs.CL]. URL: https://arxiv.org/abs/2305.10403.\n",
      "[3]Alexei Baevski et al. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. 2020.arXiv: 2006.11477 [cs.CL]. URL: https://arxiv.org/abs/2006.11477.\n",
      "[4]Jinze Bai et al. Qwen Technical Report. 2023. arXiv: 2309.16609 [cs.CL]. URL: https://arxiv.org/abs/2309.16609.\n",
      "[5]Ye Bai et al. “Seed-asr: Understanding diverse speech and contexts with llm-based speech recognition”. In: arXivpreprint arXiv:2407.04675 (2024).\n",
      "[6]James Betker. Better speech synthesis through scaling. 2023. arXiv: 2305.07243 [cs.SD]. URL: https://arxiv.org/abs/2305.07243.\n",
      "[7]Zalán Borsos et al. “Audiolm: a language modeling approach to audio generation”. In: IEEE/ACM transactionson audio, speech, and language processing 31 (2023), pp. 2523–2533.\n",
      "[8]Guoguo Chen et al. “GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of TranscribedAudio”. In: Interspeech 2021. ISCA, Aug. 2021. DOI: 10.21437/interspeech.2021-1965. URL: http://dx.doi.org/10.21437/Interspeech.2021-1965.\n",
      "[9]Qian Chen et al. “Minmo: A multimodal large language model for seamless voice interaction”. In: arXiv preprintarXiv:2501.06282 (2025).\n",
      "[10]Sanyuan Chen et al. BEATs: Audio Pre-Training with Acoustic Tokenizers. 2022. arXiv: 2212.09058 [eess.AS].URL: https://arxiv.org/abs/2212.09058.\n",
      "[11]Sanyuan Chen et al. “WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing”. In:IEEE Journal of Selected Topics in Signal Processing 16.6 (Oct. 2022), pp. 1505–1518. ISSN: 1941-0484. DOI:10.1109/jstsp.2022.3188113. URL: http://dx.doi.org/10.1109/JSTSP.2022.3188113.\n",
      "[12]Yunfei Chu et al. “Qwen-audio: Advancing universal audio understanding via unified large-scale audio-languagemodels”. In: arXiv preprint arXiv:2311.07919 (2023).\n",
      "[13]Yunfei Chu et al. “Qwen2-audio technical report”. In: arXiv preprint arXiv:2407.10759 (2024).\n",
      "[14]Jade Copet et al. Simple and Controllable Music Generation. 2024. arXiv: 2306.05284 [cs.SD]. URL: https://arxiv.org/abs/2306.05284.\n",
      "[15]Alexandre Défossez et al. “High fidelity neural audio compression”. In: arXiv preprint arXiv:2210.13438 (2022).\n",
      "[16]Alexandre Défossez et al. “Moshi: a speech-text foundation model for real-time dialogue”. In: arXiv preprintarXiv:2410.00037 (2024).\n",
      "[17]Soham Deshmukh et al. “Pengi: An audio language model for audio tasks”. In: Advances in Neural InformationProcessing Systems 36 (2023), pp. 18090–18108.\n",
      "[18]Ding Ding et al. “Kimi-audio technical report”. In: arXiv preprint arXiv:2504.18425 (2025).\n",
      "[19]Zhihao Du et al. “Cosyvoice 2: Scalable streaming speech synthesis with large language models”. In: arXivpreprint arXiv:2412.10117 (2024).\n",
      "[20]Zhihao Du et al. CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on SupervisedSemantic Tokens. 2024. arXiv: 2407.05407 [cs.SD]. URL: https://arxiv.org/abs/2407.05407.\n",
      "[21]Qingkai Fang et al. “Llama-omni: Seamless speech interaction with large language models”. In: arXiv preprintarXiv:2409.06666 (2024).\n",
      "[22]Heting Gao et al. LUCY: Linguistic Understanding and Control Yielding Early Stage of Her. 2025. arXiv:2501.16327 [cs.CL]. URL: https://arxiv.org/abs/2501.16327.\n",
      "[23]Jort F. Gemmeke et al. “Audio Set: An ontology and human-labeled dataset for audio events”. In: 2017 IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). 2017, pp. 776–780. DOI:10.1109/ICASSP.2017.7952261.\n",
      "14\n",
      "Step-Audio 2Technical Report\n",
      "[24]Sreyan Ghosh et al. Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and ExpertReasoning Abilities. 2025. arXiv: 2503.03983 [cs.SD]. URL: https://arxiv.org/abs/2503.03983.\n",
      "[25]Arushi Goel et al. Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio LanguageModels. 2025. arXiv: 2507.08128 [cs.SD]. URL: https://arxiv.org/abs/2507.08128.\n",
      "[26]Yuan Gong, Jin Yu, and James Glass. “Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition”.In: ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).2022, pp. 151–155. DOI: 10.1109/ICASSP43922.2022.9746828.\n",
      "[27]Yuan Gong et al. “Joint audio and speech understanding”. In: 2023 IEEE Automatic Speech Recognition andUnderstanding Workshop (ASRU). IEEE. 2023, pp. 1–8.\n",
      "[28]Yuan Gong et al. “Listen, think, and understand”. In: arXiv preprint arXiv:2305.10790 (2023).\n",
      "[29]Aaron Grattafiori et al. The Llama 3 Herd of Models. 2024. arXiv: 2407.21783 [cs.AI]. URL: https://arxiv.org/abs/2407.21783.\n",
      "[30]Wei-Ning Hsu et al. HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of HiddenUnits. 2021. arXiv: 2106.07447 [cs.CL]. URL: https://arxiv.org/abs/2106.07447.\n",
      "[31]Ailin Huang et al. “Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model”. In: arXivpreprint arXiv:2506.08967 (2025).\n",
      "[32]Ailin Huang et al. “Step-audio: Unified understanding and generation in intelligent speech interaction”. In: arXivpreprint arXiv:2502.11946 (2025).\n",
      "[33]Rongjie Huang et al. “Audiogpt: Understanding and generating speech, music, sound, and talking head”. In:Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 21. 2024, pp. 23802–23804.\n",
      "[34]Aaron Hurst et al. “Gpt-4o system card”. In: arXiv preprint arXiv:2410.21276 (2024).\n",
      "[35]Il-Young Jeong and Jeongsoo Park. “CochlScene: Acquisition of acoustic scene data using crowdsourcing”. In:2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC).2022, pp. 17–21. DOI: 10.23919/APSIPAASC55919.2022.9979822.\n",
      "[36]Shengpeng Ji et al. “Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling”.In: arXiv preprint arXiv:2408.16532 (2024).\n",
      "[37]Ye Jia et al. CVSS Corpus and Massively Multilingual Speech-to-Speech Translation. 2022. arXiv: 2201.03713[cs.CL]. URL: https://arxiv.org/abs/2201.03713.\n",
      "[38]Ye Jia et al. “Direct speech-to-speech translation with a sequence-to-sequence model”. In: arXiv preprintarXiv:1904.06037 (2019).\n",
      "[39]Ye Jia et al. “Translatotron 2: High-quality direct speech-to-speech translation with voice preservation”. In:International conference on machine learning. PMLR. 2022, pp. 10120–10134.\n",
      "[40]Eugene Kharitonov et al. Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision. 2023.arXiv: 2302.03540 [cs.SD]. URL: https://arxiv.org/abs/2302.03540.\n",
      "[41]Chris Dongjoo Kim et al. “Audiocaps: Generating captions for audios in the wild”. In: Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers). 2019, pp. 119–132.\n",
      "[42]Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative Adversarial Networks for Efficient andHigh Fidelity Speech Synthesis. 2020. arXiv: 2010.05646 [cs.SD]. URL: https://arxiv.org/abs/2010.05646.\n",
      "[43]Zhifeng Kong et al. Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and DialogueAbilities. 2024. arXiv: 2402.01831 [cs.SD]. URL: https://arxiv.org/abs/2402.01831.\n",
      "[44]Chenyang Le et al. “Transvip: Speech to speech translation system with voice and isochrony preservation”. In:Advances in Neural Information Processing Systems 37 (2024), pp. 89682–89705.\n",
      "[45]Ann Lee et al. “Textless speech-to-speech translation on real data”. In: arXiv preprint arXiv:2112.08352 (2021).\n",
      "[46]Sang-gil Lee et al. BigVGAN: A Universal Neural Vocoder with Large-Scale Training. 2023. arXiv: 2206.04658[cs.SD]. URL: https://arxiv.org/abs/2206.04658.\n",
      "[47]Guan-Ting Lin, Cheng-Han Chiang, and Hung-yi Lee. “Advancing large language models to capture variedspeaking styles and respond properly in spoken conversations”. In: arXiv preprint arXiv:2402.12786 (2024).\n",
      "[48]Guan-Ting Lin et al. “Paralinguistics-enhanced large language modeling of spoken dialogue”. In: ICASSP2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2024,pp. 10316–10320.\n",
      "[49]Tu Anh Nguyen et al. Spirit LM: Interleaved Spoken and Written Language Model. 2024. arXiv: 2402.05755[cs.CL]. URL: https://arxiv.org/abs/2402.05755.\n",
      "[50]OpenAI. GPT-4 Technical Report. https://openai.com/research/gpt-4. Accessed: 2025-07-11. 2023.\n",
      "15\n",
      "Step-Audio 2Technical Report\n",
      "[51]OpenAI. Introducing ChatGPT. Accessed: 2025-07-11. 2022. URL: https://openai.com/blog/chatgpt.\n",
      "[52]Wei Ping et al. “Deep voice 3: 2000-speaker neural text-to-speech”. In: proc. ICLR. Vol. 79. 2018, pp. 1094–1099.\n",
      "[53]Alec Radford et al. “Robust speech recognition via large-scale weak supervision”. In: International conferenceon machine learning. PMLR. 2023, pp. 28492–28518.\n",
      "[54]Rafael Rafailov et al. “Direct preference optimization: Your language model is secretly a reward model”. In:Advances in Neural Information Processing Systems 36 (2023), pp. 53728–53741.\n",
      "[55]Yi Ren et al. “Fastspeech 2: Fast and high-quality end-to-end text to speech”. In: arXiv preprint arXiv:2006.04558(2020).\n",
      "[56]Andrew Rouditchenko et al. “Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?” In: arXivpreprint arXiv:2505.09439 (2025).\n",
      "[57]Paul K Rubenstein et al. “Audiopalm: A large language model that can speak and listen”. In: arXiv preprintarXiv:2306.12925 (2023).\n",
      "[58]S Sakshi et al. “Mmau: A massive multi-task audio understanding and reasoning benchmark”. In: arXiv preprintarXiv:2410.19168 (2024).\n",
      "[59]Jonathan Shen et al. “Natural tts synthesis by conditioning wavenet on mel spectrogram predictions”. In: 2018IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE. 2018, pp. 4779–4783.\n",
      "[60]Hubert Siuzdak, Florian Grötschla, and Luca A Lanzendörfer. “Snac: Multi-scale neural audio codec”. In: arXivpreprint arXiv:2410.14411 (2024).\n",
      "[61]Changli Tang et al. “Salmonn: Towards generic hearing abilities for large language models”. In: arXiv preprintarXiv:2310.13289 (2023).\n",
      "[62]Wolfgang Wahlster. Verbmobil: foundations of speech-to-speech translation. Springer Science & Business Media,2013.\n",
      "[63]Changhan Wang, Anne Wu, and Juan Pino. CoVoST 2 and Massively Multilingual Speech-to-Text Translation.2020. arXiv: 2007.10310 [cs.CL]. URL: https://arxiv.org/abs/2007.10310.\n",
      "[64]Chengyi Wang et al. “Neural codec language models are zero-shot text to speech synthesizers”. In: arXiv preprintarXiv:2301.02111 (2023).\n",
      "[65]Xinsheng Wang et al. “Spark-tts: An efficient llm-based text-to-speech model with single-stream decoupledspeech tokens”. In: arXiv preprint arXiv:2503.01710 (2025).\n",
      "[66]Xiong Wang et al. “Freeze-omni: A smart and low latency speech-to-speech dialogue model with frozen llm”. In:arXiv preprint arXiv:2411.00774 (2024).\n",
      "[67]Yuancheng Wang et al. “Maskgct: Zero-shot text-to-speech with masked generative codec transformer”. In: arXivpreprint arXiv:2409.00750 (2024).\n",
      "[68]Yuxuan Wang et al. “Tacotron: Towards end-to-end speech synthesis”. In: arXiv preprint arXiv:1703.10135(2017).\n",
      "[69]Jason Wei et al. “Finetuned language models are zero-shot learners”. In: arXiv preprint arXiv:2109.01652 (2021).\n",
      "[70]Yonghui Wu et al. “Google’s neural machine translation system: Bridging the gap between human and machinetranslation”. In: arXiv preprint arXiv:1609.08144 (2016).\n",
      "[71]Zhifei Xie and Changqiao Wu. “Mini-omni: Language models can hear, talk while thinking in streaming”. In:arXiv preprint arXiv:2408.16725 (2024).\n",
      "[72]Zhifei Xie and Changqiao Wu. “Mini-omni2: Towards open-source gpt-4o with vision, speech and duplexcapabilities”. In: arXiv preprint arXiv:2410.11190 (2024).\n",
      "[73]Detai Xin et al. “Bigcodec: Pushing the limits of low-bitrate neural speech codec”. In: arXiv preprintarXiv:2409.05377 (2024).\n",
      "[74]Jin Xu et al. Qwen2.5-Omni Technical Report. 2025. arXiv: 2503.20215 [cs.CL]. URL: https://arxiv.org/abs/2503.20215.\n",
      "[75]Ruiqi Yan et al. URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue Models. 2025.arXiv: 2502.17810 [cs.CL]. URL: https://arxiv.org/abs/2502.17810.\n",
      "[76]Neil Zeghidour et al. “Soundstream: An end-to-end neural audio codec”. In: IEEE/ACM Transactions on Audio,Speech, and Language Processing 30 (2021), pp. 495–507.\n",
      "[77]Aohan Zeng et al. “Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot”. In: arXivpreprint arXiv:2412.02612 (2024).\n",
      "[78]Binbin Zhang et al. WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition.2022. arXiv: 2110.03370 [cs.SD]. URL: https://arxiv.org/abs/2110.03370.\n",
      "[79]Bowen Zhang et al. “Minimax-speech: Intrinsic zero-shot text-to-speech with a learnable speaker encoder”. In:arXiv preprint arXiv:2505.07916 (2025).\n",
      "16\n",
      "Step-Audio 2Technical Report\n",
      "[80]Dong Zhang et al. “Speechgpt: Empowering large language models with intrinsic cross-modal conversationalabilities”. In: arXiv preprint arXiv:2305.11000 (2023).\n",
      "[81]Xiangyu Zhang et al. “Distinctive Feature Codec: Adaptive Segmentation for Efficient Speech Representation”.In: arXiv preprint arXiv:2505.18516 (2025).\n",
      "[82]Xin Zhang et al. “Speechtokenizer: Unified speech tokenizer for speech large language models”. In: arXivpreprint arXiv:2308.16692 (2023).\n",
      "17\n",
      "Step-Audio 2Technical Report\n",
      "Appendix\n",
      "AContributors\n",
      "The contributors are list in alphabet order.\n",
      "A.1Core contributors\n",
      "ModelBoyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu,Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu (Tony) Zhang, XingyuanLi, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You\n",
      "InfrastructureBrian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan,Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang\n",
      "Data and evaluationBingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu,Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren,Yuankai Ma, Yufan Lu\n",
      "A.2Contributors\n",
      "Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu,Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, JianjianSun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi,Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan,Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie,Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, YuanweiLiang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang, Zidong Yang, ZixinZhang\n",
      "A.3Sponsors\n",
      "Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, YiboZhu\n",
      "A.4External contributors\n",
      "Nanyang Technological University (NTU), SingaporeEng Siong Chng, Hexin Liu\n",
      "18\n",
      "Step-Audio 2Technical Report\n",
      "BIntroduction and evaluation results of Step-Audio 2 mini\n",
      "We are pleased to release Step-Audio 2 mini, a special open-source version of Step-Audio 2,available at https://github.com/stepfun-ai/Step-Audio2. Step-Audio 2 mini employs theencoder from Qwen2-Audio as its audio encoder and is initialized with Qwen2.5-7B. Step-Audio 2mini is trained on the same dataset as Step-Audio 2, but it is limited to utilize only the web searchtool.\n",
      "Step-Audio 2 mini is a more developer-friendly variant of Step-Audio 2, with a parameter countfor fair comparisons with open-source models including Qwen-Omni and Kimi-Audio. Evaluationresults1 demonstrate that Step-Audio 2 mini delivers on par results with Step-Audio 2, exceedingmost open-source and commercial models such as GPT-4o Audio.\n",
      "B.1Automatic speech recognition\n",
      "Table 7: Comparison between Doubao LLM ASR, GPT-4o Transcribe, Kimi-Audio, Qwen-Omni, Step-Audio 2 andStep-Audio 2 mini, on character (for Chinese, Cantonese and Japanese) and word (for Arabian and English) error ratesamong multiple ASR test sets. N/A indicates that the language is not supported.\n",
      "CategoryTest setDoubaoLLM ASRGPT-4oTranscribeKimi-AudioQwen-OmniStep-Audio 2Step-Audio2 mini\n",
      "English\n",
      "Common Voice9.209.307.838.335.956.76FLEURS English7.222.714.475.053.033.05LibriSpeech clean2.921.751.492.931.171.33LibriSpeech other5.324.232.915.072.422.86\n",
      "Average6.174.504.185.353.143.50\n",
      "Chinese\n",
      "AISHELL0.983.520.641.170.630.78AISHELL-23.104.262.672.402.102.16FLEURS Chinese2.922.622.917.012.682.53KeSpeech phase16.4826.805.116.453.633.97WenetSpeech meeting4.9031.405.216.614.754.87WenetSpeech net4.4615.715.935.244.674.82\n",
      "Average3.8114.053.754.813.083.19\n",
      "FLEURS ArabianN/A11.72N/A25.1314.2216.46MultilingualCommon Voice yue9.2011.1038.907.897.908.32FLEURS JapaneseN/A3.27N/A10.493.184.67\n",
      "Anhui accent8.8350.5522.1718.7310.6111.65Guangdong accent4.997.833.764.033.814.44Guangxi accent3.377.094.293.354.113.51In-houseShanxi accent20.2655.0334.7125.9512.4415.60Sichuan dialect3.0132.855.265.614.354.57Shanghai dialect47.4989.5882.9058.7417.7719.30\n",
      "Average14.6640.4925.5219.408.859.85\n",
      "1Evaluation results are obtained with our vLLM backend and may differ from the results with transformers backend.\n",
      "19\n",
      "Step-Audio 2Technical Report\n",
      "B.2Paralinguistic information understanding\n",
      "Table 8: Comparison between GPT-4o Audio, Kimi-Audio, Qwen-Omni, Step-Audio-AQAA, Step-Audio 2 andStep-Audio 2 mini on StepEval-Audio-Paralinguistic.\n",
      "ModelAvg.GenderAgeTimbreScenarioEvent\n",
      "GPT-4o Audio43.451842342214Kimi-Audio49.649450103048Qwen-Omni44.184050162842Step-Audio-AQAA36.917066181414Step-Audio 283.0910096827860Step-Audio 2 mini80.0010094807860\n",
      "ModelEmotionPitchRhythmSpeedStyleVocal\n",
      "GPT-4o Audio824060586444Kimi-Audio665640445454Qwen-Omni763254505048Step-Audio-AQAA40384854440Step-Audio 2868286888868Step-Audio 2 mini828268748676\n",
      "B.3Audio understanding\n",
      "Table 9: Comparison between Audio Flamingo 3, Gemini 2.5 Pro, GPT-4o Audio, Kimi-Audio, Omni-R1, Qwen2.5-Omni, Step-Audio-AQAA, Step-Audio 2 and Step-Audio 2 mini on MMAU.\n",
      "ModelAvg.SoundSpeechMusic\n",
      "Audio Flamingo 373.176.966.173.9Gemini 2.5 Pro71.675.171.568.3GPT-4o Audio58.158.064.651.8Kimi-Audio69.679.065.564.4Omni-R177.081.776.073.4Qwen2.5-Omni71.578.170.665.9Step-Audio-AQAA49.750.551.447.3Step-Audio 278.083.576.973.7Step-Audio 2 mini73.276.671.571.6\n",
      "20\n",
      "Step-Audio 2Technical Report\n",
      "B.4Speech translation\n",
      "Table 10: Comparison of BLEU scores between GPT-4o Audio, Qwen2.5-Omni, Qwen-Omni, Step-Audio-AQAA,Step-Audio 2 and Step-Audio 2 mini on speech-to-text and speech-to-speech translation.\n",
      "ModelCoVoST 2 (Speech-to-text translation)\n",
      "Avg.English-to-ChineseChinese-to-English\n",
      "GPT-4o Audio29.6140.2019.01Qwen2.5-Omni35.4041.4029.40Step-Audio-AQAA28.5737.7119.43Step-Audio 239.2649.0129.51Step-Audio 2 mini39.2949.1229.47\n",
      "ModelCVSS (Speech-to-speech translation)\n",
      "Avg.English-to-ChineseChinese-to-English\n",
      "GPT-4o Audio23.6820.0727.29Qwen-Omni15.358.0422.66Step-Audio-AQAA27.3630.7423.98Step-Audio 230.8734.8326.92Step-Audio 2 mini29.0832.8125.35\n",
      "B.5Speech-to-speech conversation\n",
      "Table 11: Comparison between GPT-4o Audio, Kimi-Audio, Qwen-Omni, Step-Audio-AQAA, Step-Audio 2 andStep-Audio 2 mini on the URO-Bench. U. R. O. stands for understanding, reasoning, and oral conversation, respectively.\n",
      "ModelLanguageBasicPro\n",
      "Avg.U.R.O.Avg.U.R.O.\n",
      "GPT-4o Audio\n",
      "Chinese\n",
      "78.5989.4065.4885.2467.1070.6057.2270.20Kimi-Audio73.5979.3464.6679.7566.0760.4459.2976.21Qwen-Omni68.9859.6669.7477.2759.1159.0159.8258.74Step-Audio-AQAA74.7187.6159.6381.9365.6174.7647.2968.97Step-Audio 283.3291.0575.4586.0868.2574.7863.1865.10Step-Audio 2 mini77.8189.1964.5384.1269.5776.8458.9069.42\n",
      "GPT-4o Audio\n",
      "English\n",
      "84.5490.1875.9090.4167.5160.6564.3678.46Kimi-Audio60.0483.3642.3160.3649.7950.3240.5956.04Qwen-Omni70.5866.2969.6276.1650.9944.5163.8849.41Step-Audio-AQAA71.1190.1556.1272.0652.0144.2554.5459.81Step-Audio 283.9092.7276.5184.9266.0764.8667.7566.33Step-Audio 2 mini74.3690.0760.1277.6561.2558.7961.9463.80\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# 设置API密钥和文件ID\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "file_id = 'file-Lc7s6mAn9U'  # 替换为您的文件ID\n",
    "\n",
    "# 设置请求头和URL\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}'\n",
    "}\n",
    "url = f'https://api.stepfun.com/v1/files/{file_id}/content'\n",
    "\n",
    "# 发送请求\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 基于文档推理完整示例\n",
    "\n",
    "当提取到需要的文档内容后，可以将文档内容作为对话的输入，生成对话补全。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在上传文件...\n",
      "文件上传成功，ID: file-Lc8fPai7zU\n",
      "正在检查文件处理状态...\n",
      "文件状态: processed\n",
      "文件状态: success\n",
      "文件处理完成！\n",
      "正在获取文件内容...\n",
      "文件内容预览: Step-Audio 2 Technical Report\n",
      "StepFun Audio Team\n",
      "Abstract\n",
      "This paper presents Step-Audio 2, an end-to-end multi-modal large language modeldesigned for industry-strength audio understanding and speech ...\n",
      "提问: 请总结这份文档的主要内容\n",
      "正在生成回答...\n",
      "回答:这份文档是关于Step-Audio 2的研究报告，Step-Audio 2是一个专为工业级音频理解和语音对话设计的端到端多模态大型语言模型。文档主要介绍了Step-Audio 2的设计理念、技术实现、训练方法、评估结果等内容。\n",
      "\n",
      "1. **设计理念**：Step-Audio 2旨在通过整合潜在音频编码器和以推理为中心的强化学习（RL），实现对语音和音频的理解。同时，通过将离散音频标记的生成整合到语言建模中，实现了真正的端到端语音对话，并提高了对非语言信息（如说话风格和情感）的响应能力。\n",
      "\n",
      "2. **技术实现**：Step-Audio 2由音频编码器、音频适配器、LLM解码器和音频解tokenizer组成。音频编码器预训练在各种语音和音频理解任务上，包括自动语音识别（ASR）、说话人年龄和性别预测、音频事件检测等。音频适配器的作用是将音频编码器的输出下采样到适当的速率，以便与LLM解码器连接。LLM解码器直接将潜在音频特征作为输入，并输出交错的离散文本和音频标记序列。音频解tokenizer的作用是将音频标记转换为波形。\n",
      "\n",
      "3. **训练方法**：Step-Audio 2采用多阶段训练策略，在6800亿文本标记和800万小时的合成音频数据上进行训练。训练过程包括ASR数据预训练、音频标记整合、主要预训练和微调等阶段。\n",
      "\n",
      "4. **评估结果**：Step-Audio 2在各种语音和音频任务上取得了最先进的性能，包括ASR、音频理解、语音翻译和一般语音对话。它在多个语言的ASR测试集上取得了最低的字错误率（WER），在各种音频理解任务上也取得了最高的准确率。\n",
      "\n",
      "5. **工具调用**：Step-Audio 2还具备调用外部工具的能力，如网络搜索和音频搜索，以提供更可靠和富有表现力的响应。它还引入了音频搜索工具，这是一种新颖的工具，可以通过语音指令进行无缝语音检索，并允许模型根据检索到的语音切换音色和说话风格。\n",
      "\n",
      "6. **版本发布**：除了完整的Step-Audio 2模型，还发布了一个更适合开发者使用的版本——Step-Audio 2 mini。它使用Qwen2-Audio的编码器作为其音频编码器，并在相同的数据集上进行训练，但仅限于使用网络搜索工具。\n",
      "\n",
      "这份文档详细介绍了Step-Audio 2的设计、实现、训练和评估过程，展示了其在语音和音频理解领域的强大能力。\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# 设置API密钥\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "file_path = './media/08_step-audio2.pdf'  # 替换为您的文件路径\n",
    "\n",
    "# 设置请求头\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# 步骤1: 上传文件\n",
    "def upload_file(file_path):\n",
    "    upload_url = 'https://api.stepfun.com/v1/files'\n",
    "    files = {\n",
    "        'file': open(file_path, 'rb')\n",
    "    }\n",
    "    data = {\n",
    "        'purpose': 'file-extract'\n",
    "    }\n",
    "    response = requests.post(upload_url, headers={'Authorization': f'Bearer {STEP_API_KEY}'}, files=files, data=data)\n",
    "    return response.json()\n",
    "\n",
    "# 步骤2: 检查文件状态\n",
    "def check_file_status(file_id):\n",
    "    status_url = f'https://api.stepfun.com/v1/files/{file_id}'\n",
    "    response = requests.get(status_url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "# 步骤3: 获取文件内容\n",
    "def get_file_content(file_id):\n",
    "    content_url = f'https://api.stepfun.com/v1/files/{file_id}/content'\n",
    "    response = requests.get(content_url, headers=headers)\n",
    "    return response.text\n",
    "\n",
    "# 步骤4: 使用文件内容进行对话补全\n",
    "def chat_completion(file_content, question):\n",
    "    chat_url = 'https://api.stepfun.com/v1/chat/completions'\n",
    "    payload = {\n",
    "        'model': 'step-2-mini',  # 使用阶跃星辰的模型\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': '你是一个专业的文档分析助手，请基于提供的文档内容回答问题。'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f'文档内容：{file_content}问题：{question}'\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0.7\n",
    "    }\n",
    "    response = requests.post(chat_url, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# 执行完整流程\n",
    "try:\n",
    "    # 1. 上传文件\n",
    "    print('正在上传文件...')\n",
    "    upload_result = upload_file(file_path)\n",
    "    file_id = upload_result.get('id')\n",
    "    print(f'文件上传成功，ID: {file_id}')\n",
    "    \n",
    "    # 2. 检查文件状态\n",
    "    print('正在检查文件处理状态...')\n",
    "    max_retries = 10\n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        status_result = check_file_status(file_id)\n",
    "        status = status_result.get('status')\n",
    "        print(f'文件状态: {status}')\n",
    "        \n",
    "        if status == 'success':\n",
    "            print('文件处理完成！')\n",
    "            break\n",
    "        elif status == 'error':\n",
    "            print('文件处理失败！')\n",
    "            print(json.dumps(status_result, indent=2))\n",
    "            break\n",
    "        \n",
    "        retry_count += 1\n",
    "        time.sleep(2)  # 等待2秒后再次检查\n",
    "    \n",
    "    if status != 'success':\n",
    "        print('文件未能成功处理，无法继续。')\n",
    "        exit()\n",
    "    \n",
    "    # 3. 获取文件内容\n",
    "    print('正在获取文件内容...')\n",
    "    file_content = get_file_content(file_id)\n",
    "    content_preview = file_content[:200] + '...' if len(file_content) > 200 else file_content\n",
    "    print(f'文件内容预览: {content_preview}')\n",
    "    \n",
    "    # 4. 使用文件内容进行对话补全\n",
    "    question = '请总结这份文档的主要内容'  # 您可以替换为任何问题\n",
    "    print(f'提问: {question}')\n",
    "    print('正在生成回答...')\n",
    "    chat_result = chat_completion(file_content, question)\n",
    "    \n",
    "    # 提取并显示回答\n",
    "    answer = chat_result['choices'][0]['message']['content']\n",
    "    # answer = chat_result.get('choices', [{}])[0].get('message', {}).get('content', '无法获取回答')\n",
    "    print(f'回答:{answer}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'发生错误: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他操作实例\n",
    "\n",
    "### 查看文件列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"file-Lc7s6mAn9U\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761215391,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"file-Lc8CQZeqAK\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761215666,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"file-Lc8JYiSIkq\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761215762,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"file-Lc8MVVqUEq\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761215802,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"file-Lc8Oaht2hM\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761215831,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"file-Lc8fPai7zU\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761216058,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# 设置API密钥\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "\n",
    "# 设置请求头和URL\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}'\n",
    "}\n",
    "url = 'https://api.stepfun.com/v1/files'\n",
    "\n",
    "# 可选参数\n",
    "params = {\n",
    "    'purpose': 'file-extract',  # 按用途筛选\n",
    "    'limit': 10  # 限制返回数量\n",
    "}\n",
    "\n",
    "# 发送请求\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "# 打印响应结果\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回结果示例:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"object\": \"list\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"id\": \"file-abc123\",\n",
    "      \"object\": \"file\",\n",
    "      \"bytes\": 12345,\n",
    "      \"created_at\": 1677610602,\n",
    "      \"filename\": \"example.pdf\",\n",
    "      \"purpose\": \"file-extract\",\n",
    "      \"status\": \"success\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"file-def456\",\n",
    "      \"object\": \"file\",\n",
    "      \"bytes\": 67890,\n",
    "      \"created_at\": 1677610702,\n",
    "      \"filename\": \"another.docx\",\n",
    "      \"purpose\": \"file-extract\",\n",
    "      \"status\": \"success\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"file-Lc7s6mAn9U\",\n",
      "  \"object\": \"file\",\n",
      "  \"deleted\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# 设置API密钥和文件ID\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "file_id = 'file-Lc7s6mAn9U'  # 替换为您要删除的文件ID\n",
    "\n",
    "# 设置请求头和URL\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}'\n",
    "}\n",
    "url = f'https://api.stepfun.com/v1/files/{file_id}'\n",
    "\n",
    "# 发送删除请求\n",
    "response = requests.delete(url, headers=headers)\n",
    "\n",
    "# 打印响应结果\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回结果示例:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"file-abc123\",\n",
    "  \"object\": \"file\",\n",
    "  \"deleted\": true\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除全部文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【共发现 5 个文件】\n",
      "  - file-Lc8CQZeqAK 08_step-audio2.pdf\n",
      "  - file-Lc8JYiSIkq 08_step-audio2.pdf\n",
      "  - file-Lc8MVVqUEq 08_step-audio2.pdf\n",
      "  - file-Lc8Oaht2hM 08_step-audio2.pdf\n",
      "  - file-Lc8fPai7zU 08_step-audio2.pdf\n",
      "\n",
      "【开始批量删除】\n",
      "✅ 已删除 file-Lc8CQZeqAK\n",
      "✅ 已删除 file-Lc8JYiSIkq\n",
      "✅ 已删除 file-Lc8MVVqUEq\n",
      "✅ 已删除 file-Lc8Oaht2hM\n",
      "✅ 已删除 file-Lc8fPai7zU\n",
      "【全部删除完成】\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# 从环境变量读密钥\n",
    "STEPFUN_API_KEY = os.getenv(\"STEPFUN_API_KEY\")\n",
    "\n",
    "BASE_URL = \"https://api.stepfun.com/v1/files\"\n",
    "HEADERS  = {\"Authorization\": f\"Bearer {STEPFUN_API_KEY}\"}\n",
    "\n",
    "def list_files(purpose=\"file-extract\", limit=100):\n",
    "    \"\"\"返回文件列表 [{id, filename, purpose, ...}, ...]\"\"\"\n",
    "    params = {\"purpose\": purpose, \"limit\": limit}\n",
    "    resp = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "    resp.raise_for_status()\n",
    "    files = resp.json().get(\"data\", [])\n",
    "    print(f\"【共发现 {len(files)} 个文件】\")\n",
    "    for f in files:\n",
    "        print(\"  -\", f[\"id\"], f.get(\"filename\", \"\"))\n",
    "    return files\n",
    "\n",
    "def delete_file(file_id):\n",
    "    \"\"\"删单个文件\"\"\"\n",
    "    url = f\"{BASE_URL}/{file_id}\"\n",
    "    resp = requests.delete(url, headers=HEADERS)\n",
    "    if resp.status_code == 200:\n",
    "        print(f\"✅ 已删除 {file_id}\")\n",
    "    else:\n",
    "        print(f\"❌ 删除 {file_id} 失败\", resp.status_code, resp.text)\n",
    "\n",
    "files = list_files(purpose=\"file-extract\", limit=100)\n",
    "if not files:\n",
    "    print(\"没有需要删除的文件\")\n",
    "else:\n",
    "    print(\"\\n【开始批量删除】\")\n",
    "    for f in files:\n",
    "        delete_file(f[\"id\"])\n",
    "    print(\"【全部删除完成】\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意事项\n",
    "\n",
    "- **文件解析流程**:\n",
    "  - 对于首次解析的文件，需先查询文件解析状态，确认成功后再获取内容。\n",
    "  - 超长文本解析时，请先计算内容的 tokens 数量，以避免超出限制导致的请求中断。\n",
    "\n",
    "- **文件与请求限制**:\n",
    "  - 单文件大小：最大 64MB。\n",
    "  - 文件总数限制：最多存储 1000 个文件。\n",
    "  - 支持的意图：\n",
    "    - file-extract：用于文档内容解析。\n",
    "    - retrieval：用于知识库存储。\n",
    "\n",
    "- **支持的文件类型**:\n",
    "  - 纯文本文件：.txt, .md\n",
    "  - PDF 文件：.pdf\n",
    "  - Word 文档：.doc, .docx\n",
    "  - Excel 表格：.xls, .xlsx\n",
    "  - PPT 文件：.ppt, .pptx\n",
    "  - CSV 文件：.csv\n",
    "  - HTML/XML 文件：.html, .htm, .xml\n",
    "\n",
    "- **内容限制**:\n",
    "  - 仅支持纯文本内容解析，不支持图片或扫描的文本内容。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

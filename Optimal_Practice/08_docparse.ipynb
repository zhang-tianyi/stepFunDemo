{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æ¡£è§£æå¼€å‘æŒ‡å—\n",
    "\n",
    "é˜¶è·ƒæ˜Ÿè¾°æ–‡æ¡£è§£ææ¥å£æä¾›å¼ºå¤§çš„æ–‡æ¡£è§£æèƒ½åŠ›ï¼Œæ”¯æŒ PDFã€Word ç­‰å¤šç§æ ¼å¼ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€Ÿè§£ææ–‡æ¡£å†…å®¹ï¼Œå¹¶å°†å…¶ä½œä¸ºè¾“å…¥ç”¨äºç”Ÿæˆå’Œæ¨ç†ã€‚è¯¥æ¥å£æä¾›ä»æ–‡æ¡£å†…å®¹æå–åˆ°ç”Ÿæˆåº”ç”¨çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼Œè®©åŸºäºæ–‡æ¡£å†…å®¹çš„åº”ç”¨å¼€å‘æ›´åŠ é«˜æ•ˆä¾¿æ·ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•ˆæœ\n",
    "\n",
    "- **ä¸Šä¼ æ–‡ä»¶**ï¼šæä¾›æ–‡æ¡£ä¸Šä¼ æ¥å£ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼Œç®€åŒ–äº†åŸºäºæ–‡æ¡£çš„å†…å®¹çš„è¾“å…¥ã€‚ï¼ˆæ³¨æ„ï¼šç›®å‰ä»…æ”¯æŒçº¯æ–‡æœ¬å†…å®¹ï¼Œå›¾ç‰‡æˆ–æ‰«æå½¢å¼çš„æ–‡æœ¬å†…å®¹æš‚ä¸æ”¯æŒï¼‰\n",
    "- **æŸ¥è¯¢æ–‡ä»¶çŠ¶æ€**ï¼šæ–‡æ¡£çŠ¶æ€æŸ¥è¯¢æ¥å£è®©å¼€å‘è€…åŠæ—¶è·Ÿè¸ªè§£æè¿›åº¦ï¼Œéšæ—¶äº†è§£æ–‡æ¡£æ˜¯å¦æˆåŠŸè§£æã€‚\n",
    "- **è¯»å–æ–‡ä»¶å†…å®¹**ï¼šæ–‡æ¡£è§£ææˆåŠŸåï¼Œå†…å®¹æå–æ¥å£å¯å¿«é€Ÿè·å¾—æ–‡æ¡£å†…å®¹ï¼Œä¸ºåç»­åŸºäºæ–‡æ¡£å†…å®¹çš„ç”Ÿæˆæˆ–åˆ†ææä¾›æ•°æ®åŸºç¡€ã€‚\n",
    "- **æŸ¥è¯¢æ–‡ä»¶æ¸…å•**ï¼šæä¾›æ–‡ä»¶æ¸…å•æŸ¥è¯¢æ¥å£ï¼Œå¸®åŠ©å¼€å‘è€…è·å–å·²ä¸Šä¼ æ–‡æ¡£çš„åˆ—è¡¨ï¼Œä¾¿äºæ–‡æ¡£ç®¡ç†ã€‚\n",
    "- **åˆ é™¤æ–‡ä»¶**ï¼šæ”¯æŒæ–‡ä»¶åˆ é™¤åŠŸèƒ½ï¼Œè®©å¼€å‘è€…å¯ä»¥åœ¨æ–‡æ¡£å¤„ç†å®Œæˆåéšæ—¶åˆ é™¤æ–‡æ¡£ï¼Œæ–¹ä¾¿ç®¡ç†æ–‡æ¡£å­˜å‚¨ã€‚\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¼€å‘æ­¥éª¤\n",
    "\n",
    "![doc-parser-flow](https://platform.stepfun.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fdoc-parser-flow.bcecd2cb.jpg&w=3840&q=75)\n",
    "\n",
    "é€šè¿‡ã€ä¸Šä¼ æ–‡ä»¶ã€‘ã€æŸ¥è¯¢æ–‡ä»¶çŠ¶æ€ã€‘ã€æå–æ–‡ä»¶å†…å®¹ã€‘+æ–‡æœ¬è¡¥å…¨æ¥å£ å³å¯è·å¾—å®Œæ•´åŸºäºæ–‡æ¡£å†…å®¹çš„å¯¹è¯è¡¥å…¨èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ä¸Šä¼ æ–‡ä»¶\n",
    "\n",
    "ä¸Šä¼ æ–‡ä»¶åˆ°é˜¶è·ƒæ˜Ÿè¾°æ–‡ä»¶æœåŠ¡ï¼Œç”±é˜¶è·ƒæ˜Ÿè¾°æ–‡ä»¶æœåŠ¡è§£ææ–‡ä»¶å†…å®¹ï¼Œç”¨äºåç»­ç”Ÿæˆå’Œæ¨ç†ã€‚\n",
    "\n",
    "- ä¸Šä¼ æ—¶ä½¿ç”¨çš„ purpose å­—æ®µçš„å€¼ä¸º `file-extract`ï¼Œç”¨äºæ–‡ä»¶è§£æã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"file-Lc7s6mAn9U\",\n",
      "  \"object\": \"file\",\n",
      "  \"bytes\": 872404,\n",
      "  \"created_at\": 1761215390,\n",
      "  \"filename\": \"08_step-audio2.pdf\",\n",
      "  \"purpose\": \"file-extract\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# è®¾ç½®APIå¯†é’¥å’Œæ–‡ä»¶è·¯å¾„\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "file_path = './media/08_step-audio2.pdf'  # æ›¿æ¢ä¸ºæ‚¨çš„æ–‡ä»¶è·¯å¾„\n",
    "\n",
    "# è®¾ç½®è¯·æ±‚å¤´å’ŒURL\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}'\n",
    "}\n",
    "url = 'https://api.stepfun.com/v1/files'\n",
    "\n",
    "# å‡†å¤‡æ–‡ä»¶å’Œè¯·æ±‚å‚æ•°\n",
    "files = {\n",
    "    'file': open(file_path, 'rb')\n",
    "}\n",
    "data = {\n",
    "    'purpose': 'file-extract'  # æŒ‡å®šç”¨é€”ä¸ºæ–‡ä»¶è§£æ\n",
    "}\n",
    "\n",
    "# å‘é€è¯·æ±‚\n",
    "response = requests.post(url, headers=headers, files=files, data=data)\n",
    "\n",
    "# æ‰“å°å“åº”ç»“æœ\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿”å›ç»“æœç¤ºä¾‹:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"file-abc123\",\n",
    "  \"object\": \"file\",\n",
    "  \"bytes\": 12345,\n",
    "  \"created_at\": 1677610602,\n",
    "  \"filename\": \"example.pdf\",\n",
    "  \"purpose\": \"file-extract\",\n",
    "  \"status\": \"processing\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. æŸ¥è¯¢æ–‡ä»¶çŠ¶æ€\n",
    "\n",
    "åœ¨æ–‡ä»¶ä¸Šä¼ å®Œæˆåï¼Œéœ€è¦æŸ¥è¯¢æ–‡ä»¶è§£æçŠ¶æ€ï¼Œç¡®è®¤æ–‡ä»¶è§£ææ˜¯å¦å®Œæˆï¼Œä»¥ä¾¿åç»­è·å–æ–‡ä»¶å†…å®¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æŸ¥æ¬¡æ•° 1, çŠ¶æ€: success\n",
      "æ–‡ä»¶å¤„ç†å®Œæˆï¼\n",
      "{\n",
      "  \"id\": \"file-Lc7s6mAn9U\",\n",
      "  \"object\": \"file\",\n",
      "  \"bytes\": 872404,\n",
      "  \"created_at\": 1761215391,\n",
      "  \"filename\": \"08_step-audio2.pdf\",\n",
      "  \"purpose\": \"file-extract\",\n",
      "  \"status\": \"success\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# è®¾ç½®APIå¯†é’¥å’Œæ–‡ä»¶ID\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "file_id = 'file-Lc7s6mAn9U'  # æ›¿æ¢ä¸ºä¸Šä¼ æ–‡ä»¶åè·å¾—çš„æ–‡ä»¶ID\n",
    "\n",
    "# è®¾ç½®è¯·æ±‚å¤´å’ŒURL\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}'\n",
    "}\n",
    "url = f'https://api.stepfun.com/v1/files/{file_id}'\n",
    "\n",
    "# è½®è¯¢æ£€æŸ¥æ–‡ä»¶çŠ¶æ€ï¼Œç›´åˆ°å¤„ç†å®Œæˆ\n",
    "max_retries = 10\n",
    "retry_count = 0\n",
    "\n",
    "while retry_count < max_retries:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    result = response.json()\n",
    "    print(f'æ£€æŸ¥æ¬¡æ•° {retry_count + 1}, çŠ¶æ€: {result.get(\"status\", \"unknown\")}')\n",
    "    \n",
    "    # å¦‚æœæ–‡ä»¶å¤„ç†å®Œæˆï¼Œè·³å‡ºå¾ªç¯\n",
    "    if result.get('status') == 'success':\n",
    "        print('æ–‡ä»¶å¤„ç†å®Œæˆï¼')\n",
    "        print(json.dumps(result, indent=2))\n",
    "        break\n",
    "    \n",
    "    # å¦‚æœæ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè·³å‡ºå¾ªç¯\n",
    "    if result.get('status') == 'error':\n",
    "        print('æ–‡ä»¶å¤„ç†å¤±è´¥ï¼')\n",
    "        print(json.dumps(result, indent=2))\n",
    "        break\n",
    "    \n",
    "    # ç­‰å¾…ä¸€æ®µæ—¶é—´åå†æ¬¡æ£€æŸ¥\n",
    "    retry_count += 1\n",
    "    time.sleep(2)  # ç­‰å¾…2ç§’\n",
    "\n",
    "if retry_count >= max_retries:\n",
    "    print('è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ–‡ä»¶å¯èƒ½ä»åœ¨å¤„ç†ä¸­ã€‚')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿”å›ç»“æœç¤ºä¾‹ï¼Œå½“statusä¸ºsuccessæ—¶ï¼Œè¡¨ç¤ºæ–‡ä»¶è§£æå¤„ç†å®Œæˆ:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"file-abc123\",\n",
    "  \"object\": \"file\",\n",
    "  \"bytes\": 12345,\n",
    "  \"created_at\": 1677610602,\n",
    "  \"filename\": \"example.pdf\",\n",
    "  \"purpose\": \"file-extract\",\n",
    "  \"status\": \"success\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. æå–æ–‡ä»¶å†…å®¹\n",
    "\n",
    "å½“æ–‡ä»¶å®Œæˆè§£æåï¼Œå°±å¯ä»¥ä½¿ç”¨æå–æ–‡ä»¶å†…å®¹æ¥å£è·å–æ–‡ä»¶å†…å®¹ï¼Œå¹¶åœ¨ Chat API ä¸­ï¼Œå°†å†…å®¹ä½œä¸ºå¯¹è¯çš„è¾“å…¥ï¼Œç”Ÿæˆå¯¹è¯è¡¥å…¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step-Audio 2 Technical Report\n",
      "StepFun Audio Team\n",
      "Abstract\n",
      "This paper presents Step-Audio 2, an end-to-end multi-modal large language modeldesigned for industry-strength audio understanding and speech conversation. Byintegrating a latent audio encoder and reasoning-centric reinforcement learning(RL), Step-Audio 2 achieves promising performance in automatic speech recognition(ASR) and audio understanding. To facilitate genuine end-to-end speech conversa-tion, Step-Audio 2 incorporates the generation of discrete audio tokens into languagemodeling, significantly enhancing its responsiveness to paralinguistic informationsuch as speaking styles and emotions. To effectively leverage the rich textual andacoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmentedgeneration (RAG) and is able to call external tools such as web search to mitigatehallucination and audio search to switch timbres. Trained on millions of hours ofspeech and audio data, Step-Audio 2 delivers intelligence and expressiveness acrossdiverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2achieves state-of-the-art performance on various audio understanding and conversa-tional benchmarks compared to other open-source and commercial solutions. Pleasevisit https://github.com/stepfun-ai/Step-Audio2 for more information.\n",
      "1Introduction\n",
      "With the rapid development of large language models and audio processing technology, largeaudio language models (LALMs) have demonstrated their superiority over conventional approachesin various speech and audio processing tasks. GPT-4o is first introduced and is pioneering thedevelopment of end-to-end speech interaction without intermediate textual conversions. Subse-quently, many open-sourced LALMs [9, 13, 16, 18, 21, 31, 32, 49, 71, 72, 74, 77] are emerged,advancing multi-modal large language model capabilities in various speech and audio domains.Among these approaches, Qwen-Audio [12] and Qwen2-Audio [13] perform audio analysis andgenerate textual responses to speech instructions. Qwen2.5-Omni [74] implements a thinker-talkerarchitecture to enable full-duplex I/O during speech conversations. More recently, Kimi-Audio [18]has achieved impressive results on multiple speech and audio understanding benchmarks. In parallel,we have introduced Step-Audio [32] and Step-Audio-AQAA [31], the first LALMs to unify speechunderstanding and generation through discrete audio tokens at a scale of 130 billion parameters.\n",
      "However, existing LALMs still face challenges in achieving natural and intelligent speech interaction.Previous LALMs such as Spirit LM [49] and GLM-4-Voice [77] mainly focus on aligning thesemantic information in speech inputs to text modal, neglecting the para-linguistic informationwhich is also crucial for intentional understanding. Although LALMs including Qwen-Audio [12],Qwen2-Audio [13] and Audio Flamingo series [24, 25, 43] are capable of comprehending suchinformation, they typically generate only textual outputs and fail to further utilize this capability\n",
      "arXiv:2507.16632v3  [cs.CL]  27 Aug 2025\n",
      "Step-Audio 2Technical Report\n",
      "AISHELL-2\n",
      "LibriSpeech\n",
      "test-clean\n",
      "MMAU Speech\n",
      "MMAU Sound\n",
      "MMAU Music\n",
      "StepEval-Audio-Paralinguistic\n",
      "CoVoST 2\n",
      "(S2TT, en<->zh)\n",
      "CVSS\n",
      "(S2ST, en<->zh)\n",
      "URO-Bench\n",
      "Basic-zh\n",
      "URO-Bench\n",
      "Basic-en\n",
      "95.7\n",
      "97.3\n",
      "97.6\n",
      "97.9\n",
      "98.2\n",
      "98.5\n",
      "97.1\n",
      "98.8\n",
      "64.6\n",
      "65.5\n",
      "70.6 76.9\n",
      "58.0\n",
      "79.078.1\n",
      "83.5\n",
      "51.8\n",
      "64.4\n",
      "65.9\n",
      "73.7\n",
      "43.5\n",
      "49.6\n",
      "44.2\n",
      "83.1\n",
      "29.6\n",
      "N/A\n",
      "35.4\n",
      "39.3\n",
      "23.7\n",
      "15.3\n",
      "30.9\n",
      "78.6\n",
      "73.6\n",
      "69.0\n",
      "83.3\n",
      "84.5\n",
      "60.0\n",
      "70.6\n",
      "83.9\n",
      "GPT-4o AudioKimi-AudioQwen-OmniStep-Audio 2\n",
      "Figure 1: Performance comparision of GPT-4o Audio1, Kimi-Audio2, Qwen-Omni3 and Step-Audio 2 and on variousbenchmarks.\n",
      "to produce coherent and expressive responses in speech conversations. Moreover, due to thecomplexities of multi-modal modeling, existing LALMs frequently suffer from hallucination andoffer limited choices of timbres and speaking styles [16, 18], lacking access to real-world textualand acoustic knowledge.\n",
      "To address these issues and step into the next generation of multi-modal large language models,we present Step-Audio 2, an end-to-end large audio language model with industry-strength audioperception and speech interaction. Step-Audio 2 directly processes raw audio as input and outputsdiscrete text and audio tokens and has fewer parameters than Step-Audio [32]. Beyond capturingsemantic information in speech, the model also comprehends para-linguistic and non-vocal informa-tion in audio. By leveraging chain-of-thought (CoT) reasoning and reinforcement learning (RL),Step-Audio 2 further utilizes such multi-modal information to generate expressive speech responsescoherent to different conversation scenarios. To ground the model with real-world knowledge, Step-Audio 2 incorporates retrieval-augmented generation (RAG) and the capability to utilize variousexternal tools, including web search and audio search, to provide more reliable and expressiveresponses. Specifically, we present an audio search as a tool unique to LALMs, enabling seamlessspeech retrieval via voice instructions and allowing the model to switch timbres and speaking stylesbased on the retrieved speech.\n",
      "1GPT-4o Audio is evaluated with gpt-4o-transcribe for ASR and gpt-4o-audio-preview-2025-06-03 for others via official API.2Kimi-Audio is excluded from translation evaluations since it consistently ignores prompts.3Qwen-Omni is evaluated with Qwen2.5-Omni for MMAU and speech-to-text translation, and qwen-omni-turbo-2025-03-26 forothers via official API.\n",
      "2\n",
      "Step-Audio 2Technical Report\n",
      "4â€™â€™\n",
      "Could you talk like a little girl?\n",
      "Yay! Now I'm a happy little girl!What do you wanna talk about?\n",
      "10â€™â€™\n",
      "ğŸ‘§\n",
      "4â€™â€™\n",
      "Could you talk like a grandpa?\n",
      "Alright then, let this old manhave a chat with ya. Whatâ€™s up?\n",
      "10â€™â€™\n",
      "ğŸ‘´\n",
      "4â€™â€™\n",
      "Tell me about the Great Wall.\n",
      "Oh, the Great Wall is like agiant dragon ...\n",
      "10â€™â€™\n",
      "ğŸ‘´\n",
      "4â€™â€™\n",
      "Answer in the default voice,what is the capital of China?\n",
      "The capital of China is Beijing.\n",
      "10â€™â€™\n",
      "4â€™â€™\n",
      "(Shanghai dialect) Whatâ€™s your\n",
      "name?\n",
      "My name is Step-Audio 2.\n",
      "10â€™â€™\n",
      "4â€™â€™\n",
      "(Sichuan dialect) Whichcompany developed you?\n",
      "I'm developed by StepFun.\n",
      "10â€™â€™\n",
      "4â€™â€™\n",
      "What day is it today?\n",
      "Today is Thursday.\n",
      "10â€™â€™\n",
      "ğŸ“…\n",
      "4â€™â€™\n",
      "Please check todayâ€™s S&P 500Index.\n",
      "Todayâ€™s S&P 500 Index is6297.36.\n",
      "10â€™â€™\n",
      "ğŸ”\n",
      "4â€™â€™\n",
      "Translate, never gonna give youup, never gonna let you down.\n",
      "æ°¸è¿œä¸ä¼šæ”¾å¼ƒä½ ï¼Œæ°¸è¿œä¸ä¼šè®©ä½ å¤±æœ›ã€‚\n",
      "10â€™â€™\n",
      "4â€™â€™\n",
      "è¯·ç¿»è¯‘ä¸€ä¸‹ï¼Œé•¿åŸæ˜¯ä¸€åº§ä¼Ÿå¤§çš„å»ºç­‘ã€‚\n",
      "The Great Wall is a magnificentstructure.\n",
      "10â€™â€™\n",
      "4â€™â€™\n",
      "Could you say that sadly?\n",
      "The Great Wall is a magnificentstructure.\n",
      "10â€™â€™\n",
      "ğŸ˜­\n",
      "4â€™â€™\n",
      "Please say it in a whisper.\n",
      "The Great Wall is a magnificentstructure.\n",
      "10â€™â€™\n",
      "ğŸ¤«\n",
      "10â€™â€™\n",
      "Good eveningâ€¦ Iâ€™m feeling a bit unsettled.I wonder if I could talk to you.\n",
      "Of course, Iâ€™m here for you. It sounds like youâ€™re carrying a lot of emotions right nowâ€”that can feel overwhelming. Would you like toshare whatâ€™s on your mind, or just take a moment to unwind? Thereâ€™s no rush; Iâ€™ll listen whenever youâ€™re ready.\n",
      "25â€™â€™\n",
      "Young male, 20-25 years old, speaking in a gentle and slightly thoughtful tone, with a steady and natural pace, slightbackground noise, using standard Mandarin.\n",
      "Alright, the user has come to me now, saying â€œGood evening, Iâ€˜m feeling a bit unsettled, and Iâ€™d like to talk.â€ First, I need to showunderstanding and acceptance to make him feel safe. He may be in a state of anxiety or unease, so my response should begentle, patient, and give him ample space to express himself. The user mentioned feeling â€œa bit unsettled,â€ which suggests hemay have many emotions tangled together and isnâ€™t sure where to start. I need to guide him in slowly untangling these emotionsrather than rushing to solve the problem. At this point, empathy is keyâ€”letting him know his feelings are valid. Finally, I mustensure not to end the conversation prematurelyâ€”let the user take the lead entirely. If he doesnâ€™t want to delve deeper for now,respect his pace, offer support, and wait. Maintain a soft and caring tone, making him feel heard and supported, so heâ€™s morewilling to open up further.\n",
      "Thinking\n",
      "Figure 2: Illustration of the applications of Step-Audio 2 across various speech conversation scenarios.\n",
      "To ensure its intelligence and expressiveness in diverse conversational scenarios, we carefully designa multi-stage training strategy to train Step-Audio 2 on 680 billion tokens of text data and 8 millionhours of real and synthesized audio data. Evaluation results shown in Figure 1 demonstrate thatStep-Audio 2 achieves state-of-the-art performance in a series of audio tasks, including automaticspeech recognition (ASR) on multiple languages, audio understanding, speech-to-speech translationand speech-to-speech conversation. Typical usages of Step-Audio 2 are also illustrated in Figure 2.\n",
      "3\n",
      "Step-Audio 2Technical Report\n",
      "2Related Work\n",
      "2.1Speech and audio understanding\n",
      "Recent advances in large language models (LLMs) [4, 29, 50, 51] have extended their applicationto a wide range of speech and audio understanding tasks, such as audio captioning, sound eventdetection, automatic speech recognition, audio classification, and audio-driven creative generation.A prevalent approach [13, 17, 27, 28, 48, 61] involves pairing speech encoders with lightweight,trainable adapters that project audio features into a textual embedding space compatible with LLMs.Building on this foundation, recent studies have further explored how to incorporate paralinguisticinformation such as emotion, intonation and speaker style, enabling LLMs to move beyond purelinguistic comprehension. For instance, ParalinGPT [48] focuses on enhancing a powerful text-basedlanguage model by integrating continuous speech embeddings, enabling it to capture paralinguisticsignals such as emotion and prosody. SALMONN [61] adopts a multi-modal strategy by freezingspeech encoders Whisper [53] and BEATs [10], and connecting their outputs to an LLM via awindow-level Q-Former, enabling joint modeling of linguistic and acoustic features. Seed-ASR [5]integrates LUISE-based speech representations with instructions and context, using context-awareSFT to capture semantic information. AudioPaLM [57] combines PaLM-2 [2] and AudioLM [7],unifying linguistic knowledge with paralinguistic features like speaker identity and intonation. LLM-based approaches [12, 13] increasingly rely on pretrained audio encoders such as Wav2Vec [3],HuBERT [30], Whisper [53], and WavLM [11] to extract rich semantic representations from speech.At the same time, the extensive text knowledge and contextual reasoning capabilities stored inLLMs can provide valuable semantic guidance for understanding tasks.\n",
      "2.2Text-to-speech synthesis\n",
      "Text-to-Speech (TTS) technology has made remarkable strides in recent years, evolving fromtraditional concatenative and statistical parametric approaches [52, 55, 59, 68] to codec-based TTSsystems. Codec language models leverage a speech codec to extract discrete representations ofspeech [15, 16, 36, 60, 73, 76, 81, 82] and utilize either autoregressive [18, 80] or masked languagemodels [67] to predict the corresponding speech tokens. These tokens are then synthesized intowaveforms using codec vocoders. VALL-E [64] marked a significant breakthrough in this area. Ituses an autoregressive model to generate coarse codec codes, followed by a non-autoregressivemodel for the fine codes. Unlike VALL-E, which predicts acoustic tokens from phonemes andrequires transcripts, SPEAR-TTS [40] uses a two-stage architecture with self-supervised audioprompts to clone unseen voices from just 3 seconds of speech. SparkTTS [65] introduces BiCodec, asingle-stream speech codec that encodes linguistic content as compact semantic tokens and speakercharacteristics as fixed-length global tokens. Instead of relying on non-autoregressive models topredict residual discrete codes, methods like TorToiseTTS [6], CosyVoice [20], CosyVoice 2 [19],MiniMax-Speech [79] and SEED-TTS [1] adopt diffusion or flow-matching techniques as a secondstage to reconstruct mel-spectrograms or continuous representations enriched with fine-grainedacoustic and semantic details. Recent work, Kimi-Audio [18] combines Whisper features andsemantic tokens for efficient modeling, with dual heads and a flow-matching detokenizer plusBigVGAN [46] for low-latency, expressive synthesis.\n",
      "4\n",
      "Step-Audio 2Technical Report\n",
      "2.3Speech-to-speech translation\n",
      "Speech-to-speech translation (S2ST) is a crucial technology for eliminating communication bar-riers across languages. Traditional S2ST systems [62, 70] typically adopt a cascaded pipelineconsisting of automatic speech recognition (ASR), machine translation (MT), and TTS modules.Earlier studies [38, 39, 44, 45] have shifted toward direct approaches that bypass intermediatetextual representations, aiming for lower latency and better preservation of prosody and speakercharacteristics. Two main types of direct S2ST methods have emerged, which are known as speech-to-spectrogram translation and speech-to-unit translation. Both directly generate target speechrepresentations from the source speech without relying on textual transcriptions. A representative ofthe former is Translatotron [38], the first end-to-end model to translate source speech directly intotarget spectrograms. Translatotron 2 [39], further improves translation quality through a two-passdecoding mechanism [45]. In contrast, speech-to-unit models predict discrete acoustic tokens ratherthan spectrograms, which are typically extracted using self-supervised speech encoders such asHuBERT [30] or WavLM [11]. For instance, TransVIP [44] employs a joint encoder-decoderarchitecture that first generates target text and residual vector quantization (RVQ) codes in the initiallayer, followed by a non-causal language model that refines RVQ predictions in subsequent layers.\n",
      "2.4Speech-to-text and speech-to-speech conversation\n",
      "Based on whether the LLM can directly understand and generate speech representations, existingsystems can be categorized into end-to-end large audio language models and cascaded large audiolanguage models. The former directly models audio inputs and outputs within a unified framework,while the latter relies on a modular pipeline involving separate ASR, LLM, and TTS components.Traditional speech-to-text and speech-to-speech systems typically adopt a cascaded architecture, asexemplified by AudioGPT [33] and Spoken-LLM [47]. However, the ASR + LLM + TTS pipelineincurs high latency and modular mismatches. This has spurred interest in unified end-to-endarchitectures for faster and more seamless integration. A major milestone in this direction is GPT-4o [34], which supports direct end-to-end speech interaction without requiring intermediate textualconversions. Recently, several new end-to-end LALMs [16, 21, 71, 72, 74] for speech-to-speechconversation have emerged. For instance, Moshi [16] improves efficiency with an RQ-Transformerthat generates text and audio tokens simultaneously. Similarly, Mini-Omni [71] generates speechand text responses in parallel, following a strategy similar to MusicGen [14], which enables lowerfirst-token latency compared to interleaved generation designs. LUCY [22] builds on the Mini-Omniarchitecture with enhancements for emotional expressiveness, naturalness, and informativenessin speech generation. It utilizes curated synthetic data and optimizes the training and decodingpipelines to handle multi-turn dialogue and function-call scenarios. Mini-Omni2 [71] furtherextends Mini-Omni framework by integrating multimodal understanding and full-duplex interactioncapabilities. LLaMA-Omni [21] introduces a streaming, non-autoregressive speech decoder basedon Connectionist Temporal Classification, enabling direct and efficient generation of discrete audiotokens without relying on step-by-step prediction. Freeze-Omni [66], on the other hand, freezes theLLM parameters during training, preserving its original capabilities while achieving low-latencyspeech-to-speech interaction through streaming and decoder integration. Qwen2.5-Omni [74]supports multimodal input and simultaneous text-speech output via a thinker-talker architecture,using TMRoPE for improved audio-visual synchronization through explicit temporal encoding.\n",
      "5\n",
      "Step-Audio 2Technical Report\n",
      "Audio\n",
      "Detokenizer\n",
      "â„\n",
      "Output text-audio interleaved tokens\n",
      "â€¦\n",
      "Audio\n",
      "Encoder\n",
      "Adaptor\n",
      "â„\n",
      "Input audio features\n",
      "ğŸ”¥\n",
      "Input audioOutput speech\n",
      "LLM Decoder\n",
      "History information\n",
      "ğŸ”¥\n",
      "â€¦\n",
      "Discrete text token\n",
      "Discrete audio token\n",
      "Latent audio feature\n",
      "â€¦â€¦â€¦\n",
      "Figure 3: Architecture of the Step-Audio 2.\n",
      "3Methodology\n",
      "3.1Architecture\n",
      "Different from our previous Step-Audio [32], Step-Audio 2 further integrates the generation ofaudio tokens into language modeling, achieves end-to-end audio perception and generation. Asshown in Figure 3, Step-Audio 2 consists of an audio encoder, an audio adaptor, an LLM decoderand an audio detokenizer.\n",
      "The audio encoder is pretrained on various speech and audio understanding tasks including ASR,speaker age and gender prediction, audio event detection, etc. The audio encoder has an outputframe rate of 25 Hz and is frozen during the entire training process. An audio adaptor with adownsampling rate of 2 is employed to connect the audio encoder to LLM, thereby reducing theoutput frame rate of the audio encoder to 12.5 Hz.\n",
      "The LLM decoder directly takes the latent audio features from the audio adaptor as input, andoutputs an interleaved sequence of discrete text and audio tokens. We employ the tokenizer fromCosyVoice 2 [19] as the audio tokenizer. And the text and audio tokens are interleaved [31, 32, 77]at a fixed ratio and padded at the end to meet the ratio. The audio tokens are then extracted from theinterleaved sequence and consumed by the audio detokenizer to generate the output waveform. Theinput audio features and output interleaved sequences are then pre-filled as the history informationfor the next round of conversation.\n",
      "To provide more accurate responses and expand interactive capabilities, we design tools to retrieveaudio, current date and time, weather forecast and web content directly with explicit or implicitvoice inputs. Notably, we propose the audio search tool, a novel tool with a voice library of hundredsof thousands of speeches with their corresponding transcriptions and descriptions. With the retrievedspeech from audio search, Step-Audio 2 is able to mimic the speaking style or switching timbre\n",
      "6\n",
      "Step-Audio 2Technical Report\n",
      "according to the speech. During inference, the retrieved information is appended after the inputaudio features before generating speech outputs.\n",
      "Similar to Step-Audio [32] and Step-Audio-AQAA [31] , Step-Audio 2â€™s audio detokenizer alsoconsists of a Flow Matching module and a HiFi-GAN [42] vocoder. The Flow-Matching modulegenerates Mel spectrograms from the output audio tokens, while the vocoder further converts theMel spectrograms into waveforms. For Flow-Matching, we incorporate a CNN-based encoder layerafter each self-attention module within the transformer block and train the model on 200,000 hoursof high-quality speech. This enhancement significantly improves its Mel spectrogram reconstructioncapability, leading to substantial gains in both pronunciation accuracy and timbre similarity.\n",
      "Step-Audio 2 employs the same deployment infrastructure used in Step-Audio [32] and Step-Audio-AQAA [31], which includes a voice activity detection (VAD) module to filter out input speechesand achieves real-time voice conversation.\n",
      "3.2Pre-training\n",
      "Step-Audio 2 model is initialized with a textual LLM and then continually pre-trained on 1.356Ttokens of textual and audio data over 21 days.\n",
      "We first utilize 100B tokens of ASR data to facilitate effective alignment between speech and textfeature spaces within the adaptor. During this phase, both the audio encoder and LLM are frozen,with only the adaptor being trained. We conduct training for 12K steps at an 8,192 sequence length.And the learning rate decays from 10âˆ’4 to 2 Ã— 10âˆ’5.\n",
      "We then extend the tokenizer of the textual LLM with 6.6K audio tokens. To properly embed thenew audio tokens and preserve the modelâ€™s textual capabilities, the model is then trained on 128Btokens of text data and 128B tokens of audio data. Specifically, audio data includes 80B, 32Band 16B tokens of TTS, speech-to-speech conversation and utterance-level text-speech interleavedcontinuation data respectively. The sequence length is increased to 16,384. And the learning ratesof the LLM, adaptor, embedding layer and output layer are set to 2 Ã— 10âˆ’5, 5 Ã— 10âˆ’5, 5 Ã— 10âˆ’5,and 4 Ã— 10âˆ’5 respectively.\n",
      "We then introduce our main pre-training process and further train the model on another 800Btokens of text and audio data. We unify the learning rates to 2 Ã— 10âˆ’5 and employ 400B tokensof textual data and 42B, 120B, 8B, 30B, 5B, 45B and 150B tokens of ASR, TTS, speech-to-text translation, text-to-speech translation, speech-to-text continuation, utterance-level text-speechinterleaved continuation and speech-to-speech conversation data respectively.\n",
      "We finally employ 200B tokens of high-quality text and audio data to introduce a wider array oftasks and cooldown the model. We employ 24.6B, 12.4B, 2.4B, and 3.6B tokens of audio datafor multilingual and dialectal ASR, TTS, paralinguistic information understanding, speech-to-texttranslation respectively. Besides, we develop a conversational speech synthesis pipeline to synthesize6B, 15B and 36B tokens of audio data for speech-to-speech translation, utterance-level text-speechinterleaved conversation and speech-to-speech conversation. To ensure the vocal diversity in thesynthesized speech, the system references a library of approximately 50k unique speakers. Webalance the audio data with 100B tokens of high-quality text data and the learning rate decays from2 Ã— 10âˆ’5 to 5 Ã— 10âˆ’6.\n",
      "7\n",
      "Step-Audio 2Technical Report\n",
      "After this comprehensive pre-training procedure, the model has acquired strong audio understandingand generation capabilities while maintaining its textual performance inherited from the initialtextual LLM.\n",
      "3.3Supervised fine-tuning\n",
      "We subsequently perform a large-scale, multi-task supervised fine-tuning (SFT) procedure [69]to instruct the model to follow human intention in fluid conversations and master core tasks. Weselect audio data from open-source and proprietary data to ensure broad coverage and high quality.The model is trained on 4B tokens of text and audio data for a single epoch. And the learning ratedecays from 10âˆ’5 to 10âˆ’6.\n",
      "Specifically, we leverage extensive corpora such as GigaSpeech [8], WenetSpeech [78], and other in-house data to enhance the modelâ€™s performance in multilingual and multi-dialect ASR scenarios. Wereformat existing datasets for audio event classification and audio captioning, such as AudioSet [23]and AudioCaps [41], into speech question-answer pairs for audio understanding. To captureparalinguistic information beyond just semantics, we introduce a detailed speech captioning taskand build an in-house dataset, requiring the model to generate comprehensive textual descriptionsencompassing 11 paralinguistic and environmental aspects.\n",
      "We employ high-quality, professionally labeled data collected in-house for TTS. We utilize theChinese-to-English and English-to-Chinese subsets from the CoVoST 2 [63] dataset for speech-to-speech translation.\n",
      "We leverage high-quality in-house textual data for classic text-to-text conversation. Multiple LLMsare then employed to rewrite these text conversations as dialogue scripts with a more natural,colloquial style. We randomly insert emotion and speed instructions into the generated scriptsto enable basic emotion and speaking style control. The scripts are then synthesized into speechconversations using our conversation synthesis pipeline.\n",
      "We construct approximately 1K dialogue scripts in text for each type of external tools. Withinthese scripts, instructions with explicit or implicit tool invocation intentions and their correspondingstatements are inserted into common dialogues. The scripts are then synthesized into speechconversations using our conversation synthesis pipeline.\n",
      "Besides, we construct and employ two reasoning-centric datasets during SFT to cold-start thesubsequent reinforcement learning process. First, we build a dataset to enable and robust audiounderstanding in complex acoustic scenarios, by combining multiple audios from AudioSet andAudioCaps, thereby creating intricate acoustic environments. To better address and respond to theparalinguistic information in speech conversations, we synthesize a speech conversation dataset withour conversation synthesis pipeline, based on dialogue scripts with appropriate emotion descriptionsgenerated from textual LLMs. Subsequently, a textual LLM with reasoning capabilities is employedto produce questionâ€“answer pairs with explicit step-by-step reasoning traces, according to the audiomixing recipes or the generated dialogue scripts.\n",
      "3.4Reinforcement learning\n",
      "To enhance the modelâ€™s reasoning capabilities in audio understanding and speech interaction, weimplement a multi-stage reinforcement learning strategy. We leverage our reasoning-centric datasetsfrom SFT and utilize two stages of proximal policy optimization (PPO) [54] to optimize reasoning\n",
      "8\n",
      "Step-Audio 2Technical Report\n",
      "efficiency for real-time audio engagement. In the first stage, a binary reward function is employedto limit the thinking sequence length to a predefined maximum. This reward function assigns avalue of 1 for reasoning that is appropriately concise (neither empty nor excessively long) and 0otherwise. Training is conducted for 60 iterations with a global batch size of 64, using an actorlearning rate of 1 Ã— 10âˆ’6 and a critic learning rate of 2.5 Ã— 10âˆ’6. The second stage transitions frombinary rewards to learned preference scoring, utilizing a trained reward model to evaluate responsequality. This stage involves an additional 120 iterations while maintaining the same batch size andlearning rate settings. Finally, we incorporate group relative policy optimization (GRPO) [54] for400 iterations to further improve the modelâ€™s audio perceptual abilities.\n",
      "4Evaluation\n",
      "4.1Automatic speech recognition\n",
      "As the most critical component of audio understanding and speech interaction, we first evaluate themodelâ€™s capability in automatic speech recognition. We evaluate Step-Audio 2 across six Chinesetest sets, four English test sets, three multilingual test sets (Japanese, Cantonese, Arabic), and sixin-house Chinese dialect and accented Mandarin test sets. For comparative analysis, we utilize top-performing models from both open-source and commercial domains as baselines, including DoubaoLLM ASR1, GPT-4o Transcribe2, Kimi-Audio [18], and Qwen-Omni. We prefer GPT-4o Transcribethan GPT-4o Audio since the formers provide stronger results. Notably, Doubao LLM ASR andGPT-4o Transcribe represent specialized ASR systems that achieve leading-edge performance.\n",
      "We evaluate all the models without specifying language3 and summarize the results in Table 1. Step-Audio 2 outperforms existing open-source and commercial ASR models in both general Englishand Chinese recognition, achieving an average word error rate (WER) of 3.14% on English and anaverage character error rate (CER) of 3.08% on Chinese test sets. Moreover, Step-Audio 2 offerscomparable results to GPT-4o Transcribe on Arabian and Japanese recognition, to Qwen-Omni onCantonese recognition, demonstrating its capability in multilingual speech recognition. In addition,Step-Audio 2 achieves the lowest average CER among 4 in-house Chinese accented Mandarin and2 dialect test sets. These results highlight the superiority of Step-Audio 2 in understanding thesemantic information in speech.\n",
      "4.2Paralinguistic information understanding\n",
      "We then evaluate how Step-Audio 2 understands the paralinguistic information in speech beyondthe semantic information. To this end, we introduce StepEval-Audio-Paralinguistic, a speech-to-speech benchmark that evaluates the modelâ€™s understanding of paralinguistic information across 11dimensions using single-turn question answering.\n",
      "StepEval-Audio-Paralinguistic comprises 550 speech samples evenly distributed across 11 tasks.We initially collect 400 Chinese speech clips for 8 of these tasks from public podcast recordings,encompassing gender, age, timbre, emotion, pitch, rhythm, speaking speed, speaking style, andvocal activity prediction or description. For sound event, scenario, and vocal sound detection or\n",
      "1Doubao LLM ASR refers to https://www.volcengine.com/docs/6561/13548682GPT-4o Transcribe is evaluated using its latest model, gpt-4o-transcribe, via its official API.3We evaluate without specifying language to ensure a fair comparison. Notably, Qwen-Omni lacks a language-independenttesting approach, specifying language may yield better results.\n",
      "9\n",
      "Step-Audio 2Technical Report\n",
      "Table 1: Comparison between Doubao LLM ASR, GPT-4o Transcribe, Kimi-Audio, Qwen-Omni and Step-Audio 2, oncharacter (for Chinese, Cantonese and Japanese) and word (for Arabian and English) error rates among multiple ASRtest sets. N/A indicates that the language is not supported.\n",
      "CategoryTest setDoubaoLLM ASRGPT-4oTranscribeKimi-AudioQwen-OmniStep-Audio 2\n",
      "English\n",
      "Common Voice9.209.307.838.335.95FLEURS English7.222.714.475.053.03LibriSpeech clean2.921.751.492.931.17LibriSpeech other5.324.232.915.072.42\n",
      "Average6.174.504.185.353.14\n",
      "Chinese\n",
      "AISHELL0.983.520.641.170.63AISHELL-23.104.262.672.402.10FLEURS Chinese2.922.622.917.012.68KeSpeech phase16.4826.805.116.453.63WenetSpeech meeting4.9031.405.216.614.75WenetSpeech net4.4615.715.935.244.67\n",
      "Average3.8114.053.754.813.08\n",
      "FLEURS ArabianN/A11.72N/A25.1314.22MultilingualCommon Voice yue9.2011.1038.907.897.90FLEURS JapaneseN/A3.27N/A10.493.18\n",
      "Anhui accent8.8350.5522.1718.7310.61Guangdong accent4.997.833.764.033.81Guangxi accent3.377.094.293.354.11In-houseShanxi accent20.2655.0334.7125.9512.44Sichuan dialect3.0132.855.265.614.35Shanghai dialect47.4989.5882.9058.7417.77\n",
      "Average14.6640.4925.5219.408.85\n",
      "description, we source 50 event-related, 50 environmental, and 50 vocal sounds from AudioSet [23],CochlScene [35], and VocalSound [26], respectively. All original recordings are shorter than 30seconds and uniformly resampled to 24,000 Hz, with annotations provided by professional groupsin open-set natural language.\n",
      "We then generate textual questions and answers based on the ground-truth annotations for each taskwith textual LLMs. For the first 8 tasks, we use the input speech as a prompt to clone a synthesizedquestion speech and randomly concatenate the question before or after the original speech. Forthe remaining 3 tasks, we further mix these audios with synthesized speeches before questionconcatenation, creating more challenging test samples.\n",
      "We also establish an automatic evaluation protocol for StepEval-Audio-Paralinguistic, which initiallytranscribes model outputs into text using ASR, followed by automatic judgment with a textual LLM.More information, along with the complete StepEval-Audio-Paralinguistic test set and evaluationcode, is available at https://github.com/stepfun-ai/Step-Audio2 to foster further researchon paralinguistic information understanding.\n",
      "We evaluate GPT-4o Audio, Kimi-Audio, Qwen-Omni, Step-Audio-AQAA, and Step-Audio 2 usingthe StepEval-Audio-Paralinguistic benchmark, with results presented in Table 2. The experimentalresults highlight the comprehensive capabilities of Step-Audio 2 in understanding various paralin-guistic information, achieving an average accuracy of 83.09, which is a significant improvementover other baseline models.\n",
      "10\n",
      "Step-Audio 2Technical Report\n",
      "Table 2: Comparison between GPT-4o Audio, Kimi-Audio, Qwen-Omni, Step-Audio-AQAA and Step-Audio 2 onStepEval-Audio-Paralinguistic.\n",
      "ModelAvg.GenderAgeTimbreScenarioEvent\n",
      "GPT-4o Audio43.451842342214Kimi-Audio49.649450103048Qwen-Omni44.184050162842Step-Audio-AQAA36.917066181414Step-Audio 283.0910096827860\n",
      "ModelEmotionPitchRhythmSpeedStyleVocal\n",
      "GPT-4o Audio824060586444Kimi-Audio665640445454Qwen-Omni763254505048Step-Audio-AQAA40384854440Step-Audio 2868286888868\n",
      "4.3Audio understanding\n",
      "We then assess Step-Audio 2â€™s general audio comprehension across sound, speech, and music usingthe latest version of the MMAU benchmark [58]1.\n",
      "As baselines, we employ Audio Flamingo 3, Gemini 2.5 Pro, GPT-4o Audio, Kimi-Audio, Omni-R1 [56], Qwen2.5-Omni, and Step-Audio-AQAA. We obtain the reported results for Audio Flamingo3, Omni-R1, and Qwen2.5-Omni from their original papers. The results of Gemini 2.5 Pro areobtained from the official website of MMAU. And we re-evaluate GPT-4o Audio, Kimi-Audio andStep-Audio-AQAA due to the recent update of the MMAU benchmark.\n",
      "The results are summarized in Table 3. Step-Audio 2 achieves the highest average score of 78.0,followed by Omni-R1 and Audio Flamingo 3, both of which are specialized approaches in audiounderstanding. Specifically, Step-Audio 2 yields the best results in sound and speech tracks and onpar results with the best in music track, demonstrating its versatility and robustness across differentaudio domains.\n",
      "Table 3: Comparison between Audio Flamingo 3, Gemini 2.5 Pro, GPT-4o Audio, Kimi-Audio, Omni-R1, Qwen2.5-Omni, Step-Audio-AQAA and Step-Audio 2 on MMAU.\n",
      "ModelAvg.SoundSpeechMusic\n",
      "Audio Flamingo 373.176.966.173.9Gemini 2.5 Pro71.675.171.568.3GPT-4o Audio58.158.064.651.8Kimi-Audio69.679.065.564.4Omni-R177.081.776.073.4Qwen2.5-Omni71.578.170.665.9Step-Audio-AQAA49.750.551.447.3Step-Audio 278.083.576.973.7\n",
      "4.4Speech translation\n",
      "We evaluate the modelâ€™s bidirectional Chinese-English speech translation capabilities using twobenchmarks: speech-to-text translation (S2TT) on CoVoST 2 [63] and speech-to-speech translation\n",
      "1MMAU v05.15.25 test-mini\n",
      "11\n",
      "Step-Audio 2Technical Report\n",
      "(S2ST) on CVSS [37]. Additionally, we use the reported results of Qwen2.5-Omni for CoVoST 2,while for CVSS, we employ Qwen-Omni as a baseline. Kimi-Audio is excluded because itconsistently ignores prompts and performs ASR instead of translation. Using BLEU as the evaluationmetric, the results in Table 4 demonstrate that Step-Audio 2 achieves superior performance inChinese-English bidirectional translations, obtaining the highest average score on both the CoVoST 2and CVSS test sets.\n",
      "Table 4: Comparison of BLEU scores between GPT-4o Audio, Qwen2.5-Omni, Qwen-Omni, Step-Audio-AQAA andStep-Audio 2 on speech-to-text and speech-to-speech translation.\n",
      "ModelCoVoST 2 (Speech-to-text translation)\n",
      "Avg.English-to-ChineseChinese-to-English\n",
      "GPT-4o Audio29.6140.2019.01Qwen2.5-Omni35.4041.4029.40Step-Audio-AQAA28.5737.7119.43Step-Audio 239.2649.0129.51\n",
      "ModelCVSS (Speech-to-speech translation)\n",
      "Avg.English-to-ChineseChinese-to-English\n",
      "GPT-4o Audio23.6820.0727.29Qwen-Omni15.358.0422.66Step-Audio-AQAA27.3630.7423.98Step-Audio 230.8734.8326.92\n",
      "4.5Tool calling\n",
      "To address the gap in the availability of suitable test sets for tool calling in speech conversations, weintroduce StepEval-Audio-Toolcall, a test set that evaluates the modelâ€™s ability in tool invocation,selection and parameter extraction under Chinese speech conversations.\n",
      "We employ a textual LLM to generate 200 multi-turn dialogue scripts for each kind of tool. Eachscript contains 3-6 turns of inputs and outputs, in which previous turns may or may not include toolcalling statements, but the final input must contain a calling intention to a specific external tool.We then balance the samples with an equal number of negative samples for each kind of tools, inwhich the final speech input either has no tool calling intention or intention to call on other kindsof tools. Subsequently, we synthesize these scripts into speeches with our conversation synthesispipeline. And we propose an automatic evaluation protocol to employ Qwen3-32B to automaticallyexamine the output and tool calling statements. We release StepEval-Audio-Toolcall includingthe original scripts, synthesized speech conversations and the corresponding evaluation script inhttps://github.com/stepfun-ai/Step-Audio2.\n",
      "Despite that there is no other LALM that provides custom tool calling, we employ Qwen3-32B asa baseline to illustrate how Step-Audio 2 manages external tools in comparison to textual LLMs.As shown in Table 5, Step-Audio 2 achieves on par with tool calling accuracy with textual LLMseven with speech input. Notably, Step-Audio 2 significantly outperforms Qwen3-32B in accuratelycalling our innovative audio search tool, highlighting its specialty as a multi-modal LLM thantextual LLMs.\n",
      "12\n",
      "Step-Audio 2Technical Report\n",
      "Table 5: Comparison between Step-Audio 2 and Qwen3-32B on StepEval-Audio-Toolcall. â€ Qwen3-32B is evaluatedwith text inputs. â€¡Date and time tools have no parameter.\n",
      "ModelObjectiveMetricAudio searchDate & Timeâ€¡WeatherWeb search\n",
      "TriggerPrecision / Recall67.5 / 98.598.4 / 100.090.1 / 100.086.8 / 98.5Qwen3-32Bâ€ TypeAccuracy100.0100.098.598.5ParameterAccuracy100.0N/A100.0100.0\n",
      "TriggerPrecision / Recall86.8 / 99.596.9 / 98.492.2 / 100.088.4 / 95.5Step-Audio 2TypeAccuracy100.0100.090.598.4ParameterAccuracy100.0N/A100.0100.0\n",
      "4.6Speech-to-speech conversation\n",
      "We finally employ URO-Bench [75] to evaluate Step-Audio 2 and other open-source and commercialLALMs, including GPT-4o Audio, Kimi-Audio, Qwen-Omni, and Step-Audio-AQAA. URO-Benchconsists of multiple datasets on two difficulty tracks, evaluating the modelâ€™s understanding, reasoningand oral conversation abilities, such as ASR, instruction following, commonsense knowledge,mathematics, and speech naturalness, emotion and speaking styles expressions. We follow the ASR-mediated procedure in URO-Bench for evaluation, employing Whisper for ASR and GPT-4o-minifor automatic judging.\n",
      "As demonstrated in Table 6, Step-Audio 2 significantly outperforms existing large audio languagemodels, including GPT-4o Audio, in Chinese speech-to-speech conversation scenarios, achievingthe highest average scores of 83.32 on the basic track and 68.25 on the pro track. In Englishspeech-to-speech conversations, while Step-Audio 2 is slightly outperformed by GPT-4o Audio, itprovides very competitive results and exceeds the other approaches.\n",
      "Table 6: Comparison between GPT-4o Audio, Kimi-Audio, Qwen-Omni, Step-Audio-AQAA and Step-Audio 2 on theURO-Bench. U. R. O. stands for understanding, reasoning, and oral conversation, respectively.\n",
      "ModelLanguageBasicPro\n",
      "Avg.U.R.O.Avg.U.R.O.\n",
      "GPT-4o Audio\n",
      "Chinese\n",
      "78.5989.4065.4885.2467.1070.6057.2270.20Kimi-Audio73.5979.3464.6679.7566.0760.4459.2976.21Qwen-Omni68.9859.6669.7477.2759.1159.0159.8258.74Step-Audio-AQAA74.7187.6159.6381.9365.6174.7647.2968.97Step-Audio 283.3291.0575.4586.0868.2574.7863.1865.10\n",
      "GPT-4o Audio\n",
      "English\n",
      "84.5490.1875.9090.4167.5160.6564.3678.46Kimi-Audio60.0483.3642.3160.3649.7950.3240.5956.04Qwen-Omni70.5866.2969.6276.1650.9944.5163.8849.41Step-Audio-AQAA71.1190.1556.1272.0652.0144.2554.5459.81Step-Audio 283.9092.7276.5184.9266.0764.8667.7566.33\n",
      "5Conclusion\n",
      "We introduce Step-Audio 2, an end-to-end large audio language model designed for enterprisespeech and audio understanding, as well as intelligent speech interaction. Step-Audio 2 leverages alatent audio encoder and reinforcement learning to enhance its speech and audio comprehensioncapabilities. Furthermore, by integrating the generation of discrete audio tokens into language mod-eling, Step-Audio 2 achieves genuine end-to-end speech interaction and improves its responsiveness\n",
      "13\n",
      "Step-Audio 2Technical Report\n",
      "to paralinguistic information, such as speaking styles and emotions. Step-Audio 2 is also capable ofutilizing external tools including web search and audio search for multi-modal RAG. Trained on8 million hours of speeches and audios, Step-Audio 2 demonstrates state-of-the-art performanceacross various tasks, including ASR, audio understanding, speech translation, and general speechconversation, outperforming both open-source and commercial solutions.\n",
      "References\n",
      "[1]Philip Anastassiou et al. â€œSeed-tts: A family of high-quality versatile speech generation modelsâ€. In: arXivpreprint arXiv:2406.02430 (2024).\n",
      "[2]Rohan Anil et al. PaLM 2 Technical Report. 2023. arXiv: 2305.10403 [cs.CL]. URL: https://arxiv.org/abs/2305.10403.\n",
      "[3]Alexei Baevski et al. wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. 2020.arXiv: 2006.11477 [cs.CL]. URL: https://arxiv.org/abs/2006.11477.\n",
      "[4]Jinze Bai et al. Qwen Technical Report. 2023. arXiv: 2309.16609 [cs.CL]. URL: https://arxiv.org/abs/2309.16609.\n",
      "[5]Ye Bai et al. â€œSeed-asr: Understanding diverse speech and contexts with llm-based speech recognitionâ€. In: arXivpreprint arXiv:2407.04675 (2024).\n",
      "[6]James Betker. Better speech synthesis through scaling. 2023. arXiv: 2305.07243 [cs.SD]. URL: https://arxiv.org/abs/2305.07243.\n",
      "[7]ZalÃ¡n Borsos et al. â€œAudiolm: a language modeling approach to audio generationâ€. In: IEEE/ACM transactionson audio, speech, and language processing 31 (2023), pp. 2523â€“2533.\n",
      "[8]Guoguo Chen et al. â€œGigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of TranscribedAudioâ€. In: Interspeech 2021. ISCA, Aug. 2021. DOI: 10.21437/interspeech.2021-1965. URL: http://dx.doi.org/10.21437/Interspeech.2021-1965.\n",
      "[9]Qian Chen et al. â€œMinmo: A multimodal large language model for seamless voice interactionâ€. In: arXiv preprintarXiv:2501.06282 (2025).\n",
      "[10]Sanyuan Chen et al. BEATs: Audio Pre-Training with Acoustic Tokenizers. 2022. arXiv: 2212.09058 [eess.AS].URL: https://arxiv.org/abs/2212.09058.\n",
      "[11]Sanyuan Chen et al. â€œWavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processingâ€. In:IEEE Journal of Selected Topics in Signal Processing 16.6 (Oct. 2022), pp. 1505â€“1518. ISSN: 1941-0484. DOI:10.1109/jstsp.2022.3188113. URL: http://dx.doi.org/10.1109/JSTSP.2022.3188113.\n",
      "[12]Yunfei Chu et al. â€œQwen-audio: Advancing universal audio understanding via unified large-scale audio-languagemodelsâ€. In: arXiv preprint arXiv:2311.07919 (2023).\n",
      "[13]Yunfei Chu et al. â€œQwen2-audio technical reportâ€. In: arXiv preprint arXiv:2407.10759 (2024).\n",
      "[14]Jade Copet et al. Simple and Controllable Music Generation. 2024. arXiv: 2306.05284 [cs.SD]. URL: https://arxiv.org/abs/2306.05284.\n",
      "[15]Alexandre DÃ©fossez et al. â€œHigh fidelity neural audio compressionâ€. In: arXiv preprint arXiv:2210.13438 (2022).\n",
      "[16]Alexandre DÃ©fossez et al. â€œMoshi: a speech-text foundation model for real-time dialogueâ€. In: arXiv preprintarXiv:2410.00037 (2024).\n",
      "[17]Soham Deshmukh et al. â€œPengi: An audio language model for audio tasksâ€. In: Advances in Neural InformationProcessing Systems 36 (2023), pp. 18090â€“18108.\n",
      "[18]Ding Ding et al. â€œKimi-audio technical reportâ€. In: arXiv preprint arXiv:2504.18425 (2025).\n",
      "[19]Zhihao Du et al. â€œCosyvoice 2: Scalable streaming speech synthesis with large language modelsâ€. In: arXivpreprint arXiv:2412.10117 (2024).\n",
      "[20]Zhihao Du et al. CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on SupervisedSemantic Tokens. 2024. arXiv: 2407.05407 [cs.SD]. URL: https://arxiv.org/abs/2407.05407.\n",
      "[21]Qingkai Fang et al. â€œLlama-omni: Seamless speech interaction with large language modelsâ€. In: arXiv preprintarXiv:2409.06666 (2024).\n",
      "[22]Heting Gao et al. LUCY: Linguistic Understanding and Control Yielding Early Stage of Her. 2025. arXiv:2501.16327 [cs.CL]. URL: https://arxiv.org/abs/2501.16327.\n",
      "[23]Jort F. Gemmeke et al. â€œAudio Set: An ontology and human-labeled dataset for audio eventsâ€. In: 2017 IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP). 2017, pp. 776â€“780. DOI:10.1109/ICASSP.2017.7952261.\n",
      "14\n",
      "Step-Audio 2Technical Report\n",
      "[24]Sreyan Ghosh et al. Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and ExpertReasoning Abilities. 2025. arXiv: 2503.03983 [cs.SD]. URL: https://arxiv.org/abs/2503.03983.\n",
      "[25]Arushi Goel et al. Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio LanguageModels. 2025. arXiv: 2507.08128 [cs.SD]. URL: https://arxiv.org/abs/2507.08128.\n",
      "[26]Yuan Gong, Jin Yu, and James Glass. â€œVocalsound: A Dataset for Improving Human Vocal Sounds Recognitionâ€.In: ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).2022, pp. 151â€“155. DOI: 10.1109/ICASSP43922.2022.9746828.\n",
      "[27]Yuan Gong et al. â€œJoint audio and speech understandingâ€. In: 2023 IEEE Automatic Speech Recognition andUnderstanding Workshop (ASRU). IEEE. 2023, pp. 1â€“8.\n",
      "[28]Yuan Gong et al. â€œListen, think, and understandâ€. In: arXiv preprint arXiv:2305.10790 (2023).\n",
      "[29]Aaron Grattafiori et al. The Llama 3 Herd of Models. 2024. arXiv: 2407.21783 [cs.AI]. URL: https://arxiv.org/abs/2407.21783.\n",
      "[30]Wei-Ning Hsu et al. HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of HiddenUnits. 2021. arXiv: 2106.07447 [cs.CL]. URL: https://arxiv.org/abs/2106.07447.\n",
      "[31]Ailin Huang et al. â€œStep-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Modelâ€. In: arXivpreprint arXiv:2506.08967 (2025).\n",
      "[32]Ailin Huang et al. â€œStep-audio: Unified understanding and generation in intelligent speech interactionâ€. In: arXivpreprint arXiv:2502.11946 (2025).\n",
      "[33]Rongjie Huang et al. â€œAudiogpt: Understanding and generating speech, music, sound, and talking headâ€. In:Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 21. 2024, pp. 23802â€“23804.\n",
      "[34]Aaron Hurst et al. â€œGpt-4o system cardâ€. In: arXiv preprint arXiv:2410.21276 (2024).\n",
      "[35]Il-Young Jeong and Jeongsoo Park. â€œCochlScene: Acquisition of acoustic scene data using crowdsourcingâ€. In:2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC).2022, pp. 17â€“21. DOI: 10.23919/APSIPAASC55919.2022.9979822.\n",
      "[36]Shengpeng Ji et al. â€œWavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modelingâ€.In: arXiv preprint arXiv:2408.16532 (2024).\n",
      "[37]Ye Jia et al. CVSS Corpus and Massively Multilingual Speech-to-Speech Translation. 2022. arXiv: 2201.03713[cs.CL]. URL: https://arxiv.org/abs/2201.03713.\n",
      "[38]Ye Jia et al. â€œDirect speech-to-speech translation with a sequence-to-sequence modelâ€. In: arXiv preprintarXiv:1904.06037 (2019).\n",
      "[39]Ye Jia et al. â€œTranslatotron 2: High-quality direct speech-to-speech translation with voice preservationâ€. In:International conference on machine learning. PMLR. 2022, pp. 10120â€“10134.\n",
      "[40]Eugene Kharitonov et al. Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision. 2023.arXiv: 2302.03540 [cs.SD]. URL: https://arxiv.org/abs/2302.03540.\n",
      "[41]Chris Dongjoo Kim et al. â€œAudiocaps: Generating captions for audios in the wildâ€. In: Proceedings of the 2019Conference of the North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers). 2019, pp. 119â€“132.\n",
      "[42]Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. HiFi-GAN: Generative Adversarial Networks for Efficient andHigh Fidelity Speech Synthesis. 2020. arXiv: 2010.05646 [cs.SD]. URL: https://arxiv.org/abs/2010.05646.\n",
      "[43]Zhifeng Kong et al. Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and DialogueAbilities. 2024. arXiv: 2402.01831 [cs.SD]. URL: https://arxiv.org/abs/2402.01831.\n",
      "[44]Chenyang Le et al. â€œTransvip: Speech to speech translation system with voice and isochrony preservationâ€. In:Advances in Neural Information Processing Systems 37 (2024), pp. 89682â€“89705.\n",
      "[45]Ann Lee et al. â€œTextless speech-to-speech translation on real dataâ€. In: arXiv preprint arXiv:2112.08352 (2021).\n",
      "[46]Sang-gil Lee et al. BigVGAN: A Universal Neural Vocoder with Large-Scale Training. 2023. arXiv: 2206.04658[cs.SD]. URL: https://arxiv.org/abs/2206.04658.\n",
      "[47]Guan-Ting Lin, Cheng-Han Chiang, and Hung-yi Lee. â€œAdvancing large language models to capture variedspeaking styles and respond properly in spoken conversationsâ€. In: arXiv preprint arXiv:2402.12786 (2024).\n",
      "[48]Guan-Ting Lin et al. â€œParalinguistics-enhanced large language modeling of spoken dialogueâ€. In: ICASSP2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2024,pp. 10316â€“10320.\n",
      "[49]Tu Anh Nguyen et al. Spirit LM: Interleaved Spoken and Written Language Model. 2024. arXiv: 2402.05755[cs.CL]. URL: https://arxiv.org/abs/2402.05755.\n",
      "[50]OpenAI. GPT-4 Technical Report. https://openai.com/research/gpt-4. Accessed: 2025-07-11. 2023.\n",
      "15\n",
      "Step-Audio 2Technical Report\n",
      "[51]OpenAI. Introducing ChatGPT. Accessed: 2025-07-11. 2022. URL: https://openai.com/blog/chatgpt.\n",
      "[52]Wei Ping et al. â€œDeep voice 3: 2000-speaker neural text-to-speechâ€. In: proc. ICLR. Vol. 79. 2018, pp. 1094â€“1099.\n",
      "[53]Alec Radford et al. â€œRobust speech recognition via large-scale weak supervisionâ€. In: International conferenceon machine learning. PMLR. 2023, pp. 28492â€“28518.\n",
      "[54]Rafael Rafailov et al. â€œDirect preference optimization: Your language model is secretly a reward modelâ€. In:Advances in Neural Information Processing Systems 36 (2023), pp. 53728â€“53741.\n",
      "[55]Yi Ren et al. â€œFastspeech 2: Fast and high-quality end-to-end text to speechâ€. In: arXiv preprint arXiv:2006.04558(2020).\n",
      "[56]Andrew Rouditchenko et al. â€œOmni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?â€ In: arXivpreprint arXiv:2505.09439 (2025).\n",
      "[57]Paul K Rubenstein et al. â€œAudiopalm: A large language model that can speak and listenâ€. In: arXiv preprintarXiv:2306.12925 (2023).\n",
      "[58]S Sakshi et al. â€œMmau: A massive multi-task audio understanding and reasoning benchmarkâ€. In: arXiv preprintarXiv:2410.19168 (2024).\n",
      "[59]Jonathan Shen et al. â€œNatural tts synthesis by conditioning wavenet on mel spectrogram predictionsâ€. In: 2018IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE. 2018, pp. 4779â€“4783.\n",
      "[60]Hubert Siuzdak, Florian GrÃ¶tschla, and Luca A LanzendÃ¶rfer. â€œSnac: Multi-scale neural audio codecâ€. In: arXivpreprint arXiv:2410.14411 (2024).\n",
      "[61]Changli Tang et al. â€œSalmonn: Towards generic hearing abilities for large language modelsâ€. In: arXiv preprintarXiv:2310.13289 (2023).\n",
      "[62]Wolfgang Wahlster. Verbmobil: foundations of speech-to-speech translation. Springer Science & Business Media,2013.\n",
      "[63]Changhan Wang, Anne Wu, and Juan Pino. CoVoST 2 and Massively Multilingual Speech-to-Text Translation.2020. arXiv: 2007.10310 [cs.CL]. URL: https://arxiv.org/abs/2007.10310.\n",
      "[64]Chengyi Wang et al. â€œNeural codec language models are zero-shot text to speech synthesizersâ€. In: arXiv preprintarXiv:2301.02111 (2023).\n",
      "[65]Xinsheng Wang et al. â€œSpark-tts: An efficient llm-based text-to-speech model with single-stream decoupledspeech tokensâ€. In: arXiv preprint arXiv:2503.01710 (2025).\n",
      "[66]Xiong Wang et al. â€œFreeze-omni: A smart and low latency speech-to-speech dialogue model with frozen llmâ€. In:arXiv preprint arXiv:2411.00774 (2024).\n",
      "[67]Yuancheng Wang et al. â€œMaskgct: Zero-shot text-to-speech with masked generative codec transformerâ€. In: arXivpreprint arXiv:2409.00750 (2024).\n",
      "[68]Yuxuan Wang et al. â€œTacotron: Towards end-to-end speech synthesisâ€. In: arXiv preprint arXiv:1703.10135(2017).\n",
      "[69]Jason Wei et al. â€œFinetuned language models are zero-shot learnersâ€. In: arXiv preprint arXiv:2109.01652 (2021).\n",
      "[70]Yonghui Wu et al. â€œGoogleâ€™s neural machine translation system: Bridging the gap between human and machinetranslationâ€. In: arXiv preprint arXiv:1609.08144 (2016).\n",
      "[71]Zhifei Xie and Changqiao Wu. â€œMini-omni: Language models can hear, talk while thinking in streamingâ€. In:arXiv preprint arXiv:2408.16725 (2024).\n",
      "[72]Zhifei Xie and Changqiao Wu. â€œMini-omni2: Towards open-source gpt-4o with vision, speech and duplexcapabilitiesâ€. In: arXiv preprint arXiv:2410.11190 (2024).\n",
      "[73]Detai Xin et al. â€œBigcodec: Pushing the limits of low-bitrate neural speech codecâ€. In: arXiv preprintarXiv:2409.05377 (2024).\n",
      "[74]Jin Xu et al. Qwen2.5-Omni Technical Report. 2025. arXiv: 2503.20215 [cs.CL]. URL: https://arxiv.org/abs/2503.20215.\n",
      "[75]Ruiqi Yan et al. URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue Models. 2025.arXiv: 2502.17810 [cs.CL]. URL: https://arxiv.org/abs/2502.17810.\n",
      "[76]Neil Zeghidour et al. â€œSoundstream: An end-to-end neural audio codecâ€. In: IEEE/ACM Transactions on Audio,Speech, and Language Processing 30 (2021), pp. 495â€“507.\n",
      "[77]Aohan Zeng et al. â€œGlm-4-voice: Towards intelligent and human-like end-to-end spoken chatbotâ€. In: arXivpreprint arXiv:2412.02612 (2024).\n",
      "[78]Binbin Zhang et al. WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition.2022. arXiv: 2110.03370 [cs.SD]. URL: https://arxiv.org/abs/2110.03370.\n",
      "[79]Bowen Zhang et al. â€œMinimax-speech: Intrinsic zero-shot text-to-speech with a learnable speaker encoderâ€. In:arXiv preprint arXiv:2505.07916 (2025).\n",
      "16\n",
      "Step-Audio 2Technical Report\n",
      "[80]Dong Zhang et al. â€œSpeechgpt: Empowering large language models with intrinsic cross-modal conversationalabilitiesâ€. In: arXiv preprint arXiv:2305.11000 (2023).\n",
      "[81]Xiangyu Zhang et al. â€œDistinctive Feature Codec: Adaptive Segmentation for Efficient Speech Representationâ€.In: arXiv preprint arXiv:2505.18516 (2025).\n",
      "[82]Xin Zhang et al. â€œSpeechtokenizer: Unified speech tokenizer for speech large language modelsâ€. In: arXivpreprint arXiv:2308.16692 (2023).\n",
      "17\n",
      "Step-Audio 2Technical Report\n",
      "Appendix\n",
      "AContributors\n",
      "The contributors are list in alphabet order.\n",
      "A.1Core contributors\n",
      "ModelBoyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu,Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu (Tony) Zhang, XingyuanLi, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You\n",
      "InfrastructureBrian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan,Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang\n",
      "Data and evaluationBingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu,Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren,Yuankai Ma, Yufan Lu\n",
      "A.2Contributors\n",
      "Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu,Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, JianjianSun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi,Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan,Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie,Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, YuanweiLiang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang, Zidong Yang, ZixinZhang\n",
      "A.3Sponsors\n",
      "Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, YiboZhu\n",
      "A.4External contributors\n",
      "Nanyang Technological University (NTU), SingaporeEng Siong Chng, Hexin Liu\n",
      "18\n",
      "Step-Audio 2Technical Report\n",
      "BIntroduction and evaluation results of Step-Audio 2 mini\n",
      "We are pleased to release Step-Audio 2 mini, a special open-source version of Step-Audio 2,available at https://github.com/stepfun-ai/Step-Audio2. Step-Audio 2 mini employs theencoder from Qwen2-Audio as its audio encoder and is initialized with Qwen2.5-7B. Step-Audio 2mini is trained on the same dataset as Step-Audio 2, but it is limited to utilize only the web searchtool.\n",
      "Step-Audio 2 mini is a more developer-friendly variant of Step-Audio 2, with a parameter countfor fair comparisons with open-source models including Qwen-Omni and Kimi-Audio. Evaluationresults1 demonstrate that Step-Audio 2 mini delivers on par results with Step-Audio 2, exceedingmost open-source and commercial models such as GPT-4o Audio.\n",
      "B.1Automatic speech recognition\n",
      "Table 7: Comparison between Doubao LLM ASR, GPT-4o Transcribe, Kimi-Audio, Qwen-Omni, Step-Audio 2 andStep-Audio 2 mini, on character (for Chinese, Cantonese and Japanese) and word (for Arabian and English) error ratesamong multiple ASR test sets. N/A indicates that the language is not supported.\n",
      "CategoryTest setDoubaoLLM ASRGPT-4oTranscribeKimi-AudioQwen-OmniStep-Audio 2Step-Audio2 mini\n",
      "English\n",
      "Common Voice9.209.307.838.335.956.76FLEURS English7.222.714.475.053.033.05LibriSpeech clean2.921.751.492.931.171.33LibriSpeech other5.324.232.915.072.422.86\n",
      "Average6.174.504.185.353.143.50\n",
      "Chinese\n",
      "AISHELL0.983.520.641.170.630.78AISHELL-23.104.262.672.402.102.16FLEURS Chinese2.922.622.917.012.682.53KeSpeech phase16.4826.805.116.453.633.97WenetSpeech meeting4.9031.405.216.614.754.87WenetSpeech net4.4615.715.935.244.674.82\n",
      "Average3.8114.053.754.813.083.19\n",
      "FLEURS ArabianN/A11.72N/A25.1314.2216.46MultilingualCommon Voice yue9.2011.1038.907.897.908.32FLEURS JapaneseN/A3.27N/A10.493.184.67\n",
      "Anhui accent8.8350.5522.1718.7310.6111.65Guangdong accent4.997.833.764.033.814.44Guangxi accent3.377.094.293.354.113.51In-houseShanxi accent20.2655.0334.7125.9512.4415.60Sichuan dialect3.0132.855.265.614.354.57Shanghai dialect47.4989.5882.9058.7417.7719.30\n",
      "Average14.6640.4925.5219.408.859.85\n",
      "1Evaluation results are obtained with our vLLM backend and may differ from the results with transformers backend.\n",
      "19\n",
      "Step-Audio 2Technical Report\n",
      "B.2Paralinguistic information understanding\n",
      "Table 8: Comparison between GPT-4o Audio, Kimi-Audio, Qwen-Omni, Step-Audio-AQAA, Step-Audio 2 andStep-Audio 2 mini on StepEval-Audio-Paralinguistic.\n",
      "ModelAvg.GenderAgeTimbreScenarioEvent\n",
      "GPT-4o Audio43.451842342214Kimi-Audio49.649450103048Qwen-Omni44.184050162842Step-Audio-AQAA36.917066181414Step-Audio 283.0910096827860Step-Audio 2 mini80.0010094807860\n",
      "ModelEmotionPitchRhythmSpeedStyleVocal\n",
      "GPT-4o Audio824060586444Kimi-Audio665640445454Qwen-Omni763254505048Step-Audio-AQAA40384854440Step-Audio 2868286888868Step-Audio 2 mini828268748676\n",
      "B.3Audio understanding\n",
      "Table 9: Comparison between Audio Flamingo 3, Gemini 2.5 Pro, GPT-4o Audio, Kimi-Audio, Omni-R1, Qwen2.5-Omni, Step-Audio-AQAA, Step-Audio 2 and Step-Audio 2 mini on MMAU.\n",
      "ModelAvg.SoundSpeechMusic\n",
      "Audio Flamingo 373.176.966.173.9Gemini 2.5 Pro71.675.171.568.3GPT-4o Audio58.158.064.651.8Kimi-Audio69.679.065.564.4Omni-R177.081.776.073.4Qwen2.5-Omni71.578.170.665.9Step-Audio-AQAA49.750.551.447.3Step-Audio 278.083.576.973.7Step-Audio 2 mini73.276.671.571.6\n",
      "20\n",
      "Step-Audio 2Technical Report\n",
      "B.4Speech translation\n",
      "Table 10: Comparison of BLEU scores between GPT-4o Audio, Qwen2.5-Omni, Qwen-Omni, Step-Audio-AQAA,Step-Audio 2 and Step-Audio 2 mini on speech-to-text and speech-to-speech translation.\n",
      "ModelCoVoST 2 (Speech-to-text translation)\n",
      "Avg.English-to-ChineseChinese-to-English\n",
      "GPT-4o Audio29.6140.2019.01Qwen2.5-Omni35.4041.4029.40Step-Audio-AQAA28.5737.7119.43Step-Audio 239.2649.0129.51Step-Audio 2 mini39.2949.1229.47\n",
      "ModelCVSS (Speech-to-speech translation)\n",
      "Avg.English-to-ChineseChinese-to-English\n",
      "GPT-4o Audio23.6820.0727.29Qwen-Omni15.358.0422.66Step-Audio-AQAA27.3630.7423.98Step-Audio 230.8734.8326.92Step-Audio 2 mini29.0832.8125.35\n",
      "B.5Speech-to-speech conversation\n",
      "Table 11: Comparison between GPT-4o Audio, Kimi-Audio, Qwen-Omni, Step-Audio-AQAA, Step-Audio 2 andStep-Audio 2 mini on the URO-Bench. U. R. O. stands for understanding, reasoning, and oral conversation, respectively.\n",
      "ModelLanguageBasicPro\n",
      "Avg.U.R.O.Avg.U.R.O.\n",
      "GPT-4o Audio\n",
      "Chinese\n",
      "78.5989.4065.4885.2467.1070.6057.2270.20Kimi-Audio73.5979.3464.6679.7566.0760.4459.2976.21Qwen-Omni68.9859.6669.7477.2759.1159.0159.8258.74Step-Audio-AQAA74.7187.6159.6381.9365.6174.7647.2968.97Step-Audio 283.3291.0575.4586.0868.2574.7863.1865.10Step-Audio 2 mini77.8189.1964.5384.1269.5776.8458.9069.42\n",
      "GPT-4o Audio\n",
      "English\n",
      "84.5490.1875.9090.4167.5160.6564.3678.46Kimi-Audio60.0483.3642.3160.3649.7950.3240.5956.04Qwen-Omni70.5866.2969.6276.1650.9944.5163.8849.41Step-Audio-AQAA71.1190.1556.1272.0652.0144.2554.5459.81Step-Audio 283.9092.7276.5184.9266.0764.8667.7566.33Step-Audio 2 mini74.3690.0760.1277.6561.2558.7961.9463.80\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# è®¾ç½®APIå¯†é’¥å’Œæ–‡ä»¶ID\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "file_id = 'file-Lc7s6mAn9U'  # æ›¿æ¢ä¸ºæ‚¨çš„æ–‡ä»¶ID\n",
    "\n",
    "# è®¾ç½®è¯·æ±‚å¤´å’ŒURL\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}'\n",
    "}\n",
    "url = f'https://api.stepfun.com/v1/files/{file_id}/content'\n",
    "\n",
    "# å‘é€è¯·æ±‚\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. åŸºäºæ–‡æ¡£æ¨ç†å®Œæ•´ç¤ºä¾‹\n",
    "\n",
    "å½“æå–åˆ°éœ€è¦çš„æ–‡æ¡£å†…å®¹åï¼Œå¯ä»¥å°†æ–‡æ¡£å†…å®¹ä½œä¸ºå¯¹è¯çš„è¾“å…¥ï¼Œç”Ÿæˆå¯¹è¯è¡¥å…¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä¸Šä¼ æ–‡ä»¶...\n",
      "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ŒID: file-Lc8fPai7zU\n",
      "æ­£åœ¨æ£€æŸ¥æ–‡ä»¶å¤„ç†çŠ¶æ€...\n",
      "æ–‡ä»¶çŠ¶æ€: processed\n",
      "æ–‡ä»¶çŠ¶æ€: success\n",
      "æ–‡ä»¶å¤„ç†å®Œæˆï¼\n",
      "æ­£åœ¨è·å–æ–‡ä»¶å†…å®¹...\n",
      "æ–‡ä»¶å†…å®¹é¢„è§ˆ: Step-Audio 2 Technical Report\n",
      "StepFun Audio Team\n",
      "Abstract\n",
      "This paper presents Step-Audio 2, an end-to-end multi-modal large language modeldesigned for industry-strength audio understanding and speech ...\n",
      "æé—®: è¯·æ€»ç»“è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹\n",
      "æ­£åœ¨ç”Ÿæˆå›ç­”...\n",
      "å›ç­”:è¿™ä»½æ–‡æ¡£æ˜¯å…³äºStep-Audio 2çš„ç ”ç©¶æŠ¥å‘Šï¼ŒStep-Audio 2æ˜¯ä¸€ä¸ªä¸“ä¸ºå·¥ä¸šçº§éŸ³é¢‘ç†è§£å’Œè¯­éŸ³å¯¹è¯è®¾è®¡çš„ç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ã€‚æ–‡æ¡£ä¸»è¦ä»‹ç»äº†Step-Audio 2çš„è®¾è®¡ç†å¿µã€æŠ€æœ¯å®ç°ã€è®­ç»ƒæ–¹æ³•ã€è¯„ä¼°ç»“æœç­‰å†…å®¹ã€‚\n",
      "\n",
      "1. **è®¾è®¡ç†å¿µ**ï¼šStep-Audio 2æ—¨åœ¨é€šè¿‡æ•´åˆæ½œåœ¨éŸ³é¢‘ç¼–ç å™¨å’Œä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œå®ç°å¯¹è¯­éŸ³å’ŒéŸ³é¢‘çš„ç†è§£ã€‚åŒæ—¶ï¼Œé€šè¿‡å°†ç¦»æ•£éŸ³é¢‘æ ‡è®°çš„ç”Ÿæˆæ•´åˆåˆ°è¯­è¨€å»ºæ¨¡ä¸­ï¼Œå®ç°äº†çœŸæ­£çš„ç«¯åˆ°ç«¯è¯­éŸ³å¯¹è¯ï¼Œå¹¶æé«˜äº†å¯¹éè¯­è¨€ä¿¡æ¯ï¼ˆå¦‚è¯´è¯é£æ ¼å’Œæƒ…æ„Ÿï¼‰çš„å“åº”èƒ½åŠ›ã€‚\n",
      "\n",
      "2. **æŠ€æœ¯å®ç°**ï¼šStep-Audio 2ç”±éŸ³é¢‘ç¼–ç å™¨ã€éŸ³é¢‘é€‚é…å™¨ã€LLMè§£ç å™¨å’ŒéŸ³é¢‘è§£tokenizerç»„æˆã€‚éŸ³é¢‘ç¼–ç å™¨é¢„è®­ç»ƒåœ¨å„ç§è¯­éŸ³å’ŒéŸ³é¢‘ç†è§£ä»»åŠ¡ä¸Šï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€è¯´è¯äººå¹´é¾„å’Œæ€§åˆ«é¢„æµ‹ã€éŸ³é¢‘äº‹ä»¶æ£€æµ‹ç­‰ã€‚éŸ³é¢‘é€‚é…å™¨çš„ä½œç”¨æ˜¯å°†éŸ³é¢‘ç¼–ç å™¨çš„è¾“å‡ºä¸‹é‡‡æ ·åˆ°é€‚å½“çš„é€Ÿç‡ï¼Œä»¥ä¾¿ä¸LLMè§£ç å™¨è¿æ¥ã€‚LLMè§£ç å™¨ç›´æ¥å°†æ½œåœ¨éŸ³é¢‘ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºäº¤é”™çš„ç¦»æ•£æ–‡æœ¬å’ŒéŸ³é¢‘æ ‡è®°åºåˆ—ã€‚éŸ³é¢‘è§£tokenizerçš„ä½œç”¨æ˜¯å°†éŸ³é¢‘æ ‡è®°è½¬æ¢ä¸ºæ³¢å½¢ã€‚\n",
      "\n",
      "3. **è®­ç»ƒæ–¹æ³•**ï¼šStep-Audio 2é‡‡ç”¨å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œåœ¨6800äº¿æ–‡æœ¬æ ‡è®°å’Œ800ä¸‡å°æ—¶çš„åˆæˆéŸ³é¢‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ASRæ•°æ®é¢„è®­ç»ƒã€éŸ³é¢‘æ ‡è®°æ•´åˆã€ä¸»è¦é¢„è®­ç»ƒå’Œå¾®è°ƒç­‰é˜¶æ®µã€‚\n",
      "\n",
      "4. **è¯„ä¼°ç»“æœ**ï¼šStep-Audio 2åœ¨å„ç§è¯­éŸ³å’ŒéŸ³é¢‘ä»»åŠ¡ä¸Šå–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬ASRã€éŸ³é¢‘ç†è§£ã€è¯­éŸ³ç¿»è¯‘å’Œä¸€èˆ¬è¯­éŸ³å¯¹è¯ã€‚å®ƒåœ¨å¤šä¸ªè¯­è¨€çš„ASRæµ‹è¯•é›†ä¸Šå–å¾—äº†æœ€ä½çš„å­—é”™è¯¯ç‡ï¼ˆWERï¼‰ï¼Œåœ¨å„ç§éŸ³é¢‘ç†è§£ä»»åŠ¡ä¸Šä¹Ÿå–å¾—äº†æœ€é«˜çš„å‡†ç¡®ç‡ã€‚\n",
      "\n",
      "5. **å·¥å…·è°ƒç”¨**ï¼šStep-Audio 2è¿˜å…·å¤‡è°ƒç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œå¦‚ç½‘ç»œæœç´¢å’ŒéŸ³é¢‘æœç´¢ï¼Œä»¥æä¾›æ›´å¯é å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„å“åº”ã€‚å®ƒè¿˜å¼•å…¥äº†éŸ³é¢‘æœç´¢å·¥å…·ï¼Œè¿™æ˜¯ä¸€ç§æ–°é¢–çš„å·¥å…·ï¼Œå¯ä»¥é€šè¿‡è¯­éŸ³æŒ‡ä»¤è¿›è¡Œæ— ç¼è¯­éŸ³æ£€ç´¢ï¼Œå¹¶å…è®¸æ¨¡å‹æ ¹æ®æ£€ç´¢åˆ°çš„è¯­éŸ³åˆ‡æ¢éŸ³è‰²å’Œè¯´è¯é£æ ¼ã€‚\n",
      "\n",
      "6. **ç‰ˆæœ¬å‘å¸ƒ**ï¼šé™¤äº†å®Œæ•´çš„Step-Audio 2æ¨¡å‹ï¼Œè¿˜å‘å¸ƒäº†ä¸€ä¸ªæ›´é€‚åˆå¼€å‘è€…ä½¿ç”¨çš„ç‰ˆæœ¬â€”â€”Step-Audio 2 miniã€‚å®ƒä½¿ç”¨Qwen2-Audioçš„ç¼–ç å™¨ä½œä¸ºå…¶éŸ³é¢‘ç¼–ç å™¨ï¼Œå¹¶åœ¨ç›¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†ä»…é™äºä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·ã€‚\n",
      "\n",
      "è¿™ä»½æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†Step-Audio 2çš„è®¾è®¡ã€å®ç°ã€è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ï¼Œå±•ç¤ºäº†å…¶åœ¨è¯­éŸ³å’ŒéŸ³é¢‘ç†è§£é¢†åŸŸçš„å¼ºå¤§èƒ½åŠ›ã€‚\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# è®¾ç½®APIå¯†é’¥\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "file_path = './media/08_step-audio2.pdf'  # æ›¿æ¢ä¸ºæ‚¨çš„æ–‡ä»¶è·¯å¾„\n",
    "\n",
    "# è®¾ç½®è¯·æ±‚å¤´\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# æ­¥éª¤1: ä¸Šä¼ æ–‡ä»¶\n",
    "def upload_file(file_path):\n",
    "    upload_url = 'https://api.stepfun.com/v1/files'\n",
    "    files = {\n",
    "        'file': open(file_path, 'rb')\n",
    "    }\n",
    "    data = {\n",
    "        'purpose': 'file-extract'\n",
    "    }\n",
    "    response = requests.post(upload_url, headers={'Authorization': f'Bearer {STEP_API_KEY}'}, files=files, data=data)\n",
    "    return response.json()\n",
    "\n",
    "# æ­¥éª¤2: æ£€æŸ¥æ–‡ä»¶çŠ¶æ€\n",
    "def check_file_status(file_id):\n",
    "    status_url = f'https://api.stepfun.com/v1/files/{file_id}'\n",
    "    response = requests.get(status_url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "# æ­¥éª¤3: è·å–æ–‡ä»¶å†…å®¹\n",
    "def get_file_content(file_id):\n",
    "    content_url = f'https://api.stepfun.com/v1/files/{file_id}/content'\n",
    "    response = requests.get(content_url, headers=headers)\n",
    "    return response.text\n",
    "\n",
    "# æ­¥éª¤4: ä½¿ç”¨æ–‡ä»¶å†…å®¹è¿›è¡Œå¯¹è¯è¡¥å…¨\n",
    "def chat_completion(file_content, question):\n",
    "    chat_url = 'https://api.stepfun.com/v1/chat/completions'\n",
    "    payload = {\n",
    "        'model': 'step-2-mini',  # ä½¿ç”¨é˜¶è·ƒæ˜Ÿè¾°çš„æ¨¡å‹\n",
    "        'messages': [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ã€‚'\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f'æ–‡æ¡£å†…å®¹ï¼š{file_content}é—®é¢˜ï¼š{question}'\n",
    "            }\n",
    "        ],\n",
    "        'temperature': 0.7\n",
    "    }\n",
    "    response = requests.post(chat_url, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "# æ‰§è¡Œå®Œæ•´æµç¨‹\n",
    "try:\n",
    "    # 1. ä¸Šä¼ æ–‡ä»¶\n",
    "    print('æ­£åœ¨ä¸Šä¼ æ–‡ä»¶...')\n",
    "    upload_result = upload_file(file_path)\n",
    "    file_id = upload_result.get('id')\n",
    "    print(f'æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ŒID: {file_id}')\n",
    "    \n",
    "    # 2. æ£€æŸ¥æ–‡ä»¶çŠ¶æ€\n",
    "    print('æ­£åœ¨æ£€æŸ¥æ–‡ä»¶å¤„ç†çŠ¶æ€...')\n",
    "    max_retries = 10\n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        status_result = check_file_status(file_id)\n",
    "        status = status_result.get('status')\n",
    "        print(f'æ–‡ä»¶çŠ¶æ€: {status}')\n",
    "        \n",
    "        if status == 'success':\n",
    "            print('æ–‡ä»¶å¤„ç†å®Œæˆï¼')\n",
    "            break\n",
    "        elif status == 'error':\n",
    "            print('æ–‡ä»¶å¤„ç†å¤±è´¥ï¼')\n",
    "            print(json.dumps(status_result, indent=2))\n",
    "            break\n",
    "        \n",
    "        retry_count += 1\n",
    "        time.sleep(2)  # ç­‰å¾…2ç§’åå†æ¬¡æ£€æŸ¥\n",
    "    \n",
    "    if status != 'success':\n",
    "        print('æ–‡ä»¶æœªèƒ½æˆåŠŸå¤„ç†ï¼Œæ— æ³•ç»§ç»­ã€‚')\n",
    "        exit()\n",
    "    \n",
    "    # 3. è·å–æ–‡ä»¶å†…å®¹\n",
    "    print('æ­£åœ¨è·å–æ–‡ä»¶å†…å®¹...')\n",
    "    file_content = get_file_content(file_id)\n",
    "    content_preview = file_content[:200] + '...' if len(file_content) > 200 else file_content\n",
    "    print(f'æ–‡ä»¶å†…å®¹é¢„è§ˆ: {content_preview}')\n",
    "    \n",
    "    # 4. ä½¿ç”¨æ–‡ä»¶å†…å®¹è¿›è¡Œå¯¹è¯è¡¥å…¨\n",
    "    question = 'è¯·æ€»ç»“è¿™ä»½æ–‡æ¡£çš„ä¸»è¦å†…å®¹'  # æ‚¨å¯ä»¥æ›¿æ¢ä¸ºä»»ä½•é—®é¢˜\n",
    "    print(f'æé—®: {question}')\n",
    "    print('æ­£åœ¨ç”Ÿæˆå›ç­”...')\n",
    "    chat_result = chat_completion(file_content, question)\n",
    "    \n",
    "    # æå–å¹¶æ˜¾ç¤ºå›ç­”\n",
    "    answer = chat_result['choices'][0]['message']['content']\n",
    "    # answer = chat_result.get('choices', [{}])[0].get('message', {}).get('content', 'æ— æ³•è·å–å›ç­”')\n",
    "    print(f'å›ç­”:{answer}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'å‘ç”Ÿé”™è¯¯: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…¶ä»–æ“ä½œå®ä¾‹\n",
    "\n",
    "### æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"file-Lc7s6mAn9U\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761215391,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"file-Lc8CQZeqAK\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761215666,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"file-Lc8JYiSIkq\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761215762,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"file-Lc8MVVqUEq\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761215802,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"file-Lc8Oaht2hM\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761215831,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"file-Lc8fPai7zU\",\n",
      "      \"object\": \"file\",\n",
      "      \"bytes\": 872404,\n",
      "      \"created_at\": 1761216058,\n",
      "      \"filename\": \"08_step-audio2.pdf\",\n",
      "      \"purpose\": \"file-extract\",\n",
      "      \"status\": \"success\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# è®¾ç½®APIå¯†é’¥\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "\n",
    "# è®¾ç½®è¯·æ±‚å¤´å’ŒURL\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}'\n",
    "}\n",
    "url = 'https://api.stepfun.com/v1/files'\n",
    "\n",
    "# å¯é€‰å‚æ•°\n",
    "params = {\n",
    "    'purpose': 'file-extract',  # æŒ‰ç”¨é€”ç­›é€‰\n",
    "    'limit': 10  # é™åˆ¶è¿”å›æ•°é‡\n",
    "}\n",
    "\n",
    "# å‘é€è¯·æ±‚\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "# æ‰“å°å“åº”ç»“æœ\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿”å›ç»“æœç¤ºä¾‹:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"object\": \"list\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"id\": \"file-abc123\",\n",
    "      \"object\": \"file\",\n",
    "      \"bytes\": 12345,\n",
    "      \"created_at\": 1677610602,\n",
    "      \"filename\": \"example.pdf\",\n",
    "      \"purpose\": \"file-extract\",\n",
    "      \"status\": \"success\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"file-def456\",\n",
    "      \"object\": \"file\",\n",
    "      \"bytes\": 67890,\n",
    "      \"created_at\": 1677610702,\n",
    "      \"filename\": \"another.docx\",\n",
    "      \"purpose\": \"file-extract\",\n",
    "      \"status\": \"success\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ é™¤æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"file-Lc7s6mAn9U\",\n",
      "  \"object\": \"file\",\n",
      "  \"deleted\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# è®¾ç½®APIå¯†é’¥å’Œæ–‡ä»¶ID\n",
    "STEP_API_KEY = os.getenv(\"STEPFUN_API_KEY\") \n",
    "file_id = 'file-Lc7s6mAn9U'  # æ›¿æ¢ä¸ºæ‚¨è¦åˆ é™¤çš„æ–‡ä»¶ID\n",
    "\n",
    "# è®¾ç½®è¯·æ±‚å¤´å’ŒURL\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {STEP_API_KEY}'\n",
    "}\n",
    "url = f'https://api.stepfun.com/v1/files/{file_id}'\n",
    "\n",
    "# å‘é€åˆ é™¤è¯·æ±‚\n",
    "response = requests.delete(url, headers=headers)\n",
    "\n",
    "# æ‰“å°å“åº”ç»“æœ\n",
    "print(json.dumps(response.json(), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿”å›ç»“æœç¤ºä¾‹:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"file-abc123\",\n",
    "  \"object\": \"file\",\n",
    "  \"deleted\": true\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ é™¤å…¨éƒ¨æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã€å…±å‘ç° 5 ä¸ªæ–‡ä»¶ã€‘\n",
      "  - file-Lc8CQZeqAK 08_step-audio2.pdf\n",
      "  - file-Lc8JYiSIkq 08_step-audio2.pdf\n",
      "  - file-Lc8MVVqUEq 08_step-audio2.pdf\n",
      "  - file-Lc8Oaht2hM 08_step-audio2.pdf\n",
      "  - file-Lc8fPai7zU 08_step-audio2.pdf\n",
      "\n",
      "ã€å¼€å§‹æ‰¹é‡åˆ é™¤ã€‘\n",
      "âœ… å·²åˆ é™¤ file-Lc8CQZeqAK\n",
      "âœ… å·²åˆ é™¤ file-Lc8JYiSIkq\n",
      "âœ… å·²åˆ é™¤ file-Lc8MVVqUEq\n",
      "âœ… å·²åˆ é™¤ file-Lc8Oaht2hM\n",
      "âœ… å·²åˆ é™¤ file-Lc8fPai7zU\n",
      "ã€å…¨éƒ¨åˆ é™¤å®Œæˆã€‘\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# ä»ç¯å¢ƒå˜é‡è¯»å¯†é’¥\n",
    "STEPFUN_API_KEY = os.getenv(\"STEPFUN_API_KEY\")\n",
    "\n",
    "BASE_URL = \"https://api.stepfun.com/v1/files\"\n",
    "HEADERS  = {\"Authorization\": f\"Bearer {STEPFUN_API_KEY}\"}\n",
    "\n",
    "def list_files(purpose=\"file-extract\", limit=100):\n",
    "    \"\"\"è¿”å›æ–‡ä»¶åˆ—è¡¨ [{id, filename, purpose, ...}, ...]\"\"\"\n",
    "    params = {\"purpose\": purpose, \"limit\": limit}\n",
    "    resp = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "    resp.raise_for_status()\n",
    "    files = resp.json().get(\"data\", [])\n",
    "    print(f\"ã€å…±å‘ç° {len(files)} ä¸ªæ–‡ä»¶ã€‘\")\n",
    "    for f in files:\n",
    "        print(\"  -\", f[\"id\"], f.get(\"filename\", \"\"))\n",
    "    return files\n",
    "\n",
    "def delete_file(file_id):\n",
    "    \"\"\"åˆ å•ä¸ªæ–‡ä»¶\"\"\"\n",
    "    url = f\"{BASE_URL}/{file_id}\"\n",
    "    resp = requests.delete(url, headers=HEADERS)\n",
    "    if resp.status_code == 200:\n",
    "        print(f\"âœ… å·²åˆ é™¤ {file_id}\")\n",
    "    else:\n",
    "        print(f\"âŒ åˆ é™¤ {file_id} å¤±è´¥\", resp.status_code, resp.text)\n",
    "\n",
    "files = list_files(purpose=\"file-extract\", limit=100)\n",
    "if not files:\n",
    "    print(\"æ²¡æœ‰éœ€è¦åˆ é™¤çš„æ–‡ä»¶\")\n",
    "else:\n",
    "    print(\"\\nã€å¼€å§‹æ‰¹é‡åˆ é™¤ã€‘\")\n",
    "    for f in files:\n",
    "        delete_file(f[\"id\"])\n",
    "    print(\"ã€å…¨éƒ¨åˆ é™¤å®Œæˆã€‘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ³¨æ„äº‹é¡¹\n",
    "\n",
    "- **æ–‡ä»¶è§£ææµç¨‹**:\n",
    "  - å¯¹äºé¦–æ¬¡è§£æçš„æ–‡ä»¶ï¼Œéœ€å…ˆæŸ¥è¯¢æ–‡ä»¶è§£æçŠ¶æ€ï¼Œç¡®è®¤æˆåŠŸåå†è·å–å†…å®¹ã€‚\n",
    "  - è¶…é•¿æ–‡æœ¬è§£ææ—¶ï¼Œè¯·å…ˆè®¡ç®—å†…å®¹çš„ tokens æ•°é‡ï¼Œä»¥é¿å…è¶…å‡ºé™åˆ¶å¯¼è‡´çš„è¯·æ±‚ä¸­æ–­ã€‚\n",
    "\n",
    "- **æ–‡ä»¶ä¸è¯·æ±‚é™åˆ¶**:\n",
    "  - å•æ–‡ä»¶å¤§å°ï¼šæœ€å¤§ 64MBã€‚\n",
    "  - æ–‡ä»¶æ€»æ•°é™åˆ¶ï¼šæœ€å¤šå­˜å‚¨ 1000 ä¸ªæ–‡ä»¶ã€‚\n",
    "  - æ”¯æŒçš„æ„å›¾ï¼š\n",
    "    - file-extractï¼šç”¨äºæ–‡æ¡£å†…å®¹è§£æã€‚\n",
    "    - retrievalï¼šç”¨äºçŸ¥è¯†åº“å­˜å‚¨ã€‚\n",
    "\n",
    "- **æ”¯æŒçš„æ–‡ä»¶ç±»å‹**:\n",
    "  - çº¯æ–‡æœ¬æ–‡ä»¶ï¼š.txt, .md\n",
    "  - PDF æ–‡ä»¶ï¼š.pdf\n",
    "  - Word æ–‡æ¡£ï¼š.doc, .docx\n",
    "  - Excel è¡¨æ ¼ï¼š.xls, .xlsx\n",
    "  - PPT æ–‡ä»¶ï¼š.ppt, .pptx\n",
    "  - CSV æ–‡ä»¶ï¼š.csv\n",
    "  - HTML/XML æ–‡ä»¶ï¼š.html, .htm, .xml\n",
    "\n",
    "- **å†…å®¹é™åˆ¶**:\n",
    "  - ä»…æ”¯æŒçº¯æ–‡æœ¬å†…å®¹è§£æï¼Œä¸æ”¯æŒå›¾ç‰‡æˆ–æ‰«æçš„æ–‡æœ¬å†…å®¹ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
